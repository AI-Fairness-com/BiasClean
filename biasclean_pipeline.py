# -*- coding: utf-8 -*-
"""biasclean_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qLX7g6WAgUSgwxm6Sb-z2zQ7BwtVFRqa
"""

"""
Local BiasClean pipeline wrapper for three-step use:
1) biasclean_detect
2) biasclean_remove
3) biasclean_report
4) biasclean_visualize
"""

import pandas as pd
import matplotlib.pyplot as plt
import json
from pathlib import Path
from typing import Dict, Any, Tuple
import numpy as np

# Adjust this import to match your actual module/package name
from biasclean_v2 import BiasClean, comprehensive_statistical_diagnosis, BiasCleanVisualizer


def _load_df(input_path: str) -> pd.DataFrame:
    input_path = Path(input_path)
    if not input_path.exists():
        raise FileNotFoundError(f"Input file not found: {input_path}")
    return pd.read_csv(input_path)


def biasclean_detect(input_path: str, domain: str) -> Dict[str, Any]:
    """
    Step 1: Load dataset and compute baseline bias score for a given domain.
    Returns a dict with basic stats for quick inspection in terminal.
    """
    df = _load_df(input_path)

    bc = BiasClean()
    bc.fit(domain)

    score_before = bc.score(df)
    
    # Run comprehensive diagnosis to get detailed bias information
    diagnostic_results = comprehensive_statistical_diagnosis(df, bc.domain_weights)

    result = {
        "domain": domain,
        "n_rows": int(len(df)),
        "n_cols": int(df.shape[1]),
        "bias_score_before": float(score_before),
        "significant_biases_detected": diagnostic_results['significant_bias_count'],
        "requires_mitigation": diagnostic_results['requires_mitigation'],
        "dataset_size": diagnostic_results['dataset_size'],
        "most_biased_feature": max(diagnostic_results['feature_tests'].items(), 
                                 key=lambda x: x[1]['effect_size'])[0] if diagnostic_results['feature_tests'] else "None"
    }
    return result


def biasclean_remove(
    input_path: str,
    domain: str,
    mode: str = "industry",
    output_path: str = "biasclean_corrected.csv",
) -> Tuple[Dict[str, Any], str]:
    """
    Step 2: Apply BiasClean transformation and save corrected dataset.
    Returns (summary_dict, output_path).
    """
    df = _load_df(input_path)

    bc = BiasClean()
    bc.fit(domain)

    # Run comprehensive statistical diagnosis first (REQUIRED for v2 API)
    diagnostic_results = comprehensive_statistical_diagnosis(df, bc.domain_weights)
    
    score_before = bc.score(df)
    
    # Apply transformation with diagnostic results
    if mode == "industry":
        df_corrected = bc.transform_industry(df, diagnostic_results)
    else:
        df_corrected = bc.transform(df, mode=mode, diagnostic_results=diagnostic_results)

    n_before = len(df)
    n_after = len(df_corrected)
    data_loss = 0.0 if n_before == 0 else (n_before - n_after) / n_before

    output_path = str(Path(output_path).resolve())
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    df_corrected.to_csv(output_path, index=False)

    # Calculate improvement metrics
    score_after = bc.score(df_corrected)
    bias_reduction = ((score_before - score_after) / score_before) * 100 if score_before > 0 else 0

    # Get industry validation for comprehensive metrics
    industry_validation = bc.validate_industry_readiness(df, df_corrected, diagnostic_results)

    summary = {
        "domain": domain,
        "mode": mode,
        "n_rows_before": int(n_before),
        "n_rows_after": int(n_after),
        "data_loss_percent": float(data_loss * 100),
        "data_retention_percent": 100 - float(data_loss * 100),
        "bias_score_before": float(score_before),
        "bias_score_after": float(score_after),
        "bias_reduction_percent": float(bias_reduction),
        "significant_biases_found": diagnostic_results['significant_bias_count'],
        "production_ready": industry_validation['industry_metrics']['production_ready'],
        "average_fairness_improvement": float(np.mean(list(industry_validation['fairness_improvement'].values())) if industry_validation['fairness_improvement'] else 0)
    }
    return summary, output_path


def biasclean_report(
    input_path: str,
    domain: str,
    mode: str = "industry",
    report_path: str = "biasclean_report.txt",
    corrected_path: str | None = None,
) -> str:
    """
    Step 3: Generate a textual report comparing original vs corrected dataset.
    If corrected_path is given, it will be loaded; otherwise it will be created on the fly.
    Returns the report_path used.
    """
    df = _load_df(input_path)

    bc = BiasClean()
    bc.fit(domain)

    if corrected_path is not None and Path(corrected_path).exists():
        df_corrected = pd.read_csv(corrected_path)
    else:
        # Run diagnosis and transformation if no corrected file provided
        diagnostic_results = comprehensive_statistical_diagnosis(df, bc.domain_weights)
        if mode == "industry":
            df_corrected = bc.transform_industry(df, diagnostic_results)
        else:
            df_corrected = bc.transform(df, mode=mode, diagnostic_results=diagnostic_results)

    # Generate comprehensive report
    report_dict = bc.report(df, df_corrected)
    
    # Get industry validation for additional metrics
    diagnostic_results = comprehensive_statistical_diagnosis(df, bc.domain_weights)
    industry_validation = bc.validate_industry_readiness(df, df_corrected, diagnostic_results)
    
    # Convert dict to readable text report
    report_text = f"BIASCLEAN V2.0 - COMPREHENSIVE FAIRNESS REPORT\n"
    report_text += "=" * 70 + "\n\n"
    
    report_text += f"DOMAIN: {report_dict['domain'].upper()}\n"
    report_text += f"TIMESTAMP: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
    report_text += f"MODE: {mode.upper()}\n\n"
    
    report_text += "EXECUTIVE SUMMARY:\n"
    report_text += "-" * 40 + "\n"
    report_text += f"âœ“ Bias Reduction: {report_dict['reduction_percent']:.1f}%\n"
    report_text += f"âœ“ Data Retention: {100 - report_dict['data_loss_percent']:.1f}%\n"
    report_text += f"âœ“ Production Ready: {'âœ… YES' if industry_validation['industry_metrics']['production_ready'] else 'âŒ NO'}\n\n"
    
    report_text += "DATASET METRICS:\n"
    report_text += "-" * 40 + "\n"
    report_text += f"Records Before: {report_dict['records_before']:,}\n"
    report_text += f"Records After:  {report_dict['records_after']:,}\n"
    report_text += f"Data Retention: {100 - report_dict['data_loss_percent']:.1f}%\n"
    report_text += f"Data Loss:      {report_dict['data_loss_percent']:.1f}%\n\n"
    
    report_text += "BIAS SCORES:\n"
    report_text += "-" * 40 + "\n"
    report_text += f"Initial Bias Score: {report_dict['bias_score_before']:.4f}\n"
    report_text += f"Final Bias Score:   {report_dict['bias_score_after']:.4f}\n"
    report_text += f"Overall Reduction:  {report_dict['reduction_percent']:.1f}%\n\n"
    
    report_text += "FAIRNESS IMPROVEMENTS:\n"
    report_text += "-" * 40 + "\n"
    if industry_validation['fairness_improvement']:
        for feature, improvement in industry_validation['fairness_improvement'].items():
            status = "âœ…" if improvement > 15 else "âš ï¸ "
            report_text += f"  {status} {feature}: {improvement:+.1f}% closer to uniform\n"
    report_text += "\n"
    
    report_text += "FEATURE WEIGHTS (Domain-Specific):\n"
    report_text += "-" * 40 + "\n"
    for feature, weight in report_dict['feature_weights'].items():
        report_text += f"  {feature}: {weight:.3f}\n"
    report_text += "\n"
    
    if report_dict.get('detected_biases'):
        report_text += "DETECTED BIASES:\n"
        report_text += "-" * 40 + "\n"
        for feature, bias_info in report_dict['detected_biases'].items():
            if bias_info['weighted_impact'] > 0.01:  # Only show significant biases
                report_text += f"  ğŸš¨ {feature}:\n"
                report_text += f"     Disparity Score: {bias_info['disparity_score']:.4f}\n"
                report_text += f"     Weighted Impact: {bias_info['weighted_impact']:.4f}\n"
                if bias_info['over_represented_groups']:
                    report_text += f"     Over-represented Groups: {len(bias_info['over_represented_groups'])}\n"
        report_text += "\n"
    
    report_text += "INDUSTRY VALIDATION:\n"
    report_text += "-" * 40 + "\n"
    report_text += f"Data Retention: {industry_validation['data_integrity']['retention_rate']:.1f}%\n"
    report_text += f"Meets Retention Target (â‰¥92%): {'âœ… YES' if industry_validation['industry_metrics']['meets_data_retention'] else 'âŒ NO'}\n"
    report_text += f"Meaningful Fairness Gain (>15%): {'âœ… YES' if industry_validation['industry_metrics']['meaningful_fairness_gain'] else 'âŒ NO'}\n"
    report_text += f"Production Ready: {'âœ… YES' if industry_validation['industry_metrics']['production_ready'] else 'âŒ NO'}\n\n"
    
    report_text += "MITIGATION EFFICIENCY:\n"
    report_text += "-" * 40 + "\n"
    report_text += f"Efficiency Score: {report_dict['mitigation_efficiency']:.2f}\n\n"
    
    report_text += "=" * 70 + "\n"
    report_text += "END OF REPORT - BIASCLEAN V2.0 | PRODUCTION-READY FAIRNESS ENGINE\n"

    report_path = str(Path(report_path).resolve())
    Path(report_path).parent.mkdir(parents=True, exist_ok=True)
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report_text)

    return report_path


def biasclean_visualize(
    input_path: str,
    corrected_path: str,
    domain: str,
    output_dir: str = "biasclean_visualizations"
) -> str:
    """
    Step 4: Generate professional visualizations for bias mitigation results.
    Returns the directory path where visualizations are saved.
    """
    df_before = _load_df(input_path)
    df_after = _load_df(corrected_path)

    bc = BiasClean()
    bc.fit(domain)
    
    # Create visualizer instance
    visualizer = BiasCleanVisualizer(bc)
    
    # Create output directory
    output_dir_path = Path(output_dir)
    output_dir_path.mkdir(parents=True, exist_ok=True)
    
    print("ğŸ“Š GENERATING PROFESSIONAL VISUALIZATIONS...")
    print("-" * 50)
    
    try:
        # 1. Comprehensive report (main dashboard)
        print("ğŸ”„ Creating comprehensive dashboard...")
        fig1 = visualizer.create_comprehensive_report(df_before, df_after)
        fig1.savefig(output_dir_path / "01_comprehensive_dashboard.png", dpi=300, bbox_inches='tight', facecolor='white')
        plt.close(fig1)
        print("   âœ… Comprehensive dashboard saved")
        
        # 2. Feature distribution comparisons
        print("ğŸ”„ Creating feature distribution plots...")
        fig2 = visualizer.plot_feature_distributions(df_before, df_after)
        fig2.savefig(output_dir_path / "02_feature_distributions.png", dpi=300, bbox_inches='tight', facecolor='white')
        plt.close(fig2)
        print("   âœ… Feature distributions saved")
        
        # 3. Bias score comparison chart (SAFE VERSION)
        print("ğŸ”„ Creating bias score comparison...")
        plt.figure(figsize=(12, 8))
        
        # Calculate scores with bounds checking
        score_before = max(0, bc.score(df_before))  # Ensure non-negative
        score_after = max(0, bc.score(df_after))    # Ensure non-negative
        
        # Safe reduction calculation
        if score_before > 0:
            reduction = ((score_before - score_after) / score_before) * 100
            reduction = max(-100, min(100, reduction))  # Bound between -100% and +100%
        else:
            reduction = 0
        
        # Create professional bar chart
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
        
        # Main bias score comparison
        bars = ax1.bar(['Before Mitigation', 'After Mitigation'], 
                      [score_before, score_after], 
                      color=['#e74c3c', '#2ecc71'], alpha=0.8, width=0.6)
        ax1.set_title('Bias Score Reduction', fontweight='bold', fontsize=16, pad=20)
        ax1.set_ylabel('Bias Score\n(Lower = Fairer)', fontweight='bold')
        ax1.grid(True, alpha=0.3, axis='y')
        
        # Add value labels on bars
        for bar, score in zip(bars, [score_before, score_after]):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=12)
        
        # Reduction percentage with safe bounds
        safe_reduction = max(-100, min(100, reduction))  # Ensure within reasonable bounds
        ax2.bar(['Bias Reduction'], [safe_reduction], color='#3498db', alpha=0.8, width=0.4)
        ax2.set_title('Overall Improvement', fontweight='bold', fontsize=16, pad=20)
        ax2.set_ylabel('Reduction (%)', fontweight='bold')
        
        # Dynamic y-axis limits for reduction chart
        y_min = min(0, safe_reduction * 1.1) if safe_reduction < 0 else 0
        y_max = max(100, safe_reduction * 1.1) if safe_reduction > 0 else 100
        ax2.set_ylim(y_min, y_max)
        
        ax2.grid(True, alpha=0.3, axis='y')
        ax2.text(0, safe_reduction + (2 if safe_reduction >= 0 else -2), 
                f'{safe_reduction:.1f}%', ha='center', va='bottom' if safe_reduction >= 0 else 'top', 
                fontweight='bold', fontsize=14, color='#2c3e50')
        
        plt.tight_layout()
        plt.savefig(output_dir_path / "03_bias_score_comparison.png", dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        print("   âœ… Bias score comparison saved")
        
        # 4. Data retention metrics (FIXED for all scenarios)
        print("ğŸ”„ Creating data retention visualization...")
        plt.figure(figsize=(10, 8))
        
        retention = (len(df_after) / len(df_before)) * 100
        data_change = retention - 100  # Positive = growth, Negative = loss
        
        # Safe pie chart data preparation
        if data_change >= 0:
            # Data growth case
            sizes = [100, min(data_change, 200)]  # Cap growth at 200% for visualization
            labels = [f'Original Data\n100%', f'Data Growth\n+{min(data_change, 200):.1f}%']
            colors = ['#3498db', '#2ecc71']
            explode = (0, 0.1)
            title = 'Data Growth Metrics'
        else:
            # Data loss case
            sizes = [max(retention, 0), min(abs(data_change), 100)]  # Ensure non-negative
            labels = [f'Data Retained\n{max(retention, 0):.1f}%', f'Data Loss\n{min(abs(data_change), 100):.1f}%']
            colors = ['#2ecc71', '#e74c3c']
            explode = (0.1, 0)
            title = 'Data Retention Metrics'
        
        # Final safety check for pie chart values
        sizes = [max(0, size) for size in sizes]  # Ensure all values non-negative
        total = sum(sizes)
        if total > 0:
            # Normalize to 100% if needed
            sizes = [size * 100 / total for size in sizes]
        
        plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',
                shadow=True, startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})
        plt.axis('equal')
        plt.title(title, fontweight='bold', fontsize=16, pad=20)
        
        plt.savefig(output_dir_path / "04_data_retention.png", dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        print("   âœ… Data retention visualization saved")
        
        # 5. Industry readiness dashboard (FIXED with comprehensive safety)
        print("ğŸ”„ Creating industry readiness dashboard...")
        diagnostic_results = comprehensive_statistical_diagnosis(df_before, bc.domain_weights)
        industry_validation = bc.validate_industry_readiness(df_before, df_after, diagnostic_results)
        
        plt.figure(figsize=(14, 10))
        
        # Industry metrics with comprehensive bounds checking
        metrics = ['Data Retention', 'Fairness Gain', 'Production Ready']
        
        # Safe score calculations with bounds
        retention_score = max(0, min(100, industry_validation['data_integrity']['retention_rate']))
        
        fairness_values = list(industry_validation['fairness_improvement'].values()) if industry_validation['fairness_improvement'] else [0]
        fairness_score = max(0, min(100, np.mean(fairness_values)))
        
        production_score = 100 if industry_validation['industry_metrics']['production_ready'] else 0
        
        scores = [retention_score, fairness_score, production_score]
        colors = ['#3498db', '#2ecc71', '#9b59b6']
        
        # Create gauge chart style with guaranteed safe values
        fig, axes = plt.subplots(1, 3, figsize=(16, 6))
        
        for idx, (metric, score, color) in enumerate(zip(metrics, scores, colors)):
            ax = axes[idx]
            
            # Draw gauge with guaranteed safe values
            wedge_params = {'width': 0.3, 'alpha': 0.8}
            safe_score = max(0, min(100, score))  # Double-check bounds
            remaining = max(0, 100 - safe_score)  # Ensure non-negative
            
            # Final safety check before pie chart
            pie_sizes = [safe_score, remaining]
            if sum(pie_sizes) == 0:
                pie_sizes = [50, 50]  # Default if somehow both are zero
            
            ax.pie(pie_sizes, startangle=90, colors=[color, '#ecf0f1'], 
                  wedgeprops=wedge_params)
            ax.text(0, 0, f'{score:.1f}%', ha='center', va='center', 
                   fontweight='bold', fontsize=16)
            ax.set_title(f'{metric}\n', fontweight='bold', fontsize=14)
        
        plt.suptitle('INDUSTRY READINESS DASHBOARD', fontweight='bold', fontsize=18, y=1.02)
        plt.tight_layout()
        plt.savefig(output_dir_path / "05_industry_readiness.png", dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        print("   âœ… Industry readiness dashboard saved")
        
        # Cleanup
        visualizer.close_all_figures()
        
        print("-" * 50)
        print(f"âœ… ALL VISUALIZATIONS SUCCESSFULLY GENERATED!")
        print(f"ğŸ“ Location: {output_dir_path}/")
        print(f"ğŸ“ Files created:")
        print(f"   01_comprehensive_dashboard.png")
        print(f"   02_feature_distributions.png") 
        print(f"   03_bias_score_comparison.png")
        print(f"   04_data_retention.png")
        print(f"   05_industry_readiness.png")
        
    except Exception as e:
        print(f"âŒ Error generating visualizations: {e}")
        # Cleanup on error
        visualizer.close_all_figures()
        raise
    
    return str(output_dir_path)


def biasclean_validate(
    input_path: str,
    corrected_path: str,
    domain: str,
    validation_path: str = "biasclean_validation.json"
) -> str:
    """
    Additional step: Comprehensive validation of bias mitigation results.
    Returns industry-ready validation metrics.
    """
    df_before = _load_df(input_path)
    df_after = _load_df(corrected_path)

    bc = BiasClean()
    bc.fit(domain)

    # Run diagnosis on original data
    diagnostic_results = comprehensive_statistical_diagnosis(df_before, bc.domain_weights)
    
    # Run industry validation
    validation_results = bc.validate_industry_readiness(df_before, df_after, diagnostic_results)
    
    validation_path = str(Path(validation_path).resolve())
    Path(validation_path).parent.mkdir(parents=True, exist_ok=True)
    
    with open(validation_path, "w", encoding="utf-8") as f:
        json.dump(validation_results, f, indent=2, default=str)
    
    return validation_path


def biasclean_full_pipeline(
    input_path: str,
    domain: str,
    mode: str = "industry",
    output_prefix: str = "biasclean_results"
) -> Dict[str, Any]:
    """
    Complete pipeline: detect â†’ remove â†’ report â†’ visualize â†’ validate
    Returns comprehensive results dictionary.
    """
    print("ğŸš€ LAUNCHING COMPLETE BIASCLEAN PIPELINE")
    print("=" * 60)
    
    results = {}
    
    try:
        # 1. Detection
        print("ğŸ”¬ STEP 1: Bias Detection")
        detect_results = biasclean_detect(input_path, domain)
        results['detection'] = detect_results
        print(f"   âœ“ Initial bias score: {detect_results['bias_score_before']:.4f}")
        
        # 2. Removal
        print("ğŸ”„ STEP 2: Bias Mitigation")
        corrected_path = f"{output_prefix}_corrected.csv"
        remove_results, corrected_file = biasclean_remove(input_path, domain, mode, corrected_path)
        results['removal'] = remove_results
        print(f"   âœ“ Bias reduction: {remove_results['bias_reduction_percent']:.1f}%")
        print(f"   âœ“ Data retention: {remove_results['data_retention_percent']:.1f}%")
        
        # 3. Reporting
        print("ğŸ“Š STEP 3: Report Generation")
        report_path = f"{output_prefix}_report.txt"
        report_file = biasclean_report(input_path, domain, mode, report_path, corrected_file)
        results['reporting'] = {'report_path': report_file}
        print(f"   âœ“ Report saved: {report_file}")
        
        # 4. Visualization
        print("ğŸ¨ STEP 4: Professional Visualization")
        viz_dir = biasclean_visualize(input_path, corrected_file, domain, f"{output_prefix}_visualizations")
        results['visualization'] = {'visualization_dir': viz_dir}
        print(f"   âœ“ Visualizations saved: {viz_dir}")
        
        # 5. Validation
        print("âœ… STEP 5: Industry Validation")
        validation_path = f"{output_prefix}_validation.json"
        validate_file = biasclean_validate(input_path, corrected_file, domain, validation_path)
        results['validation'] = {'validation_path': validate_file}
        print(f"   âœ“ Validation saved: {validate_file}")
        
        print("=" * 60)
        print("ğŸ‰ COMPLETE PIPELINE EXECUTED SUCCESSFULLY!")
        print(f"ğŸ“ All results saved with prefix: {output_prefix}_*")
        
    except Exception as e:
        print(f"âŒ Pipeline execution failed: {e}")
        raise
    
    return results