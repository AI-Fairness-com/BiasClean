# -*- coding: utf-8 -*-
"""BiasClean_v2.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L0_Gh0RqiM9suM_FDQzAkZ24jUJEvE9J
"""

"""
BiasClean v2.0 - Domain-Specific Bias Detection and Mitigation Toolkit
UK-Focused(2025) Fairness Engine for Structured Data Pre-processing

Author: Hamid Tavakoli
Version: 2.0
Date: 2025
License: HT

Official GitHub: https://github.com/AI-Fairness-com/BiasClean

DESCRIPTION:
BiasClean v2.0 introduces a domain-specific, region-specific multi-feature
fairness engine that detects and mitigates representational bias in structured
datasets prior to modelling. The system is UK-focused and built on seven
fairness-critical domains with seven universal fairness features.

REFERENCE:
BiasClean v2.0 Project Report - Feature Discovery, Domain Analysis &
Evidence-Based Weight Construction (UK 2025)

CITATION:
If using this software, please cite the methodology and weight matrix from
the official BiasClean v2.0 technical documentation.
"""

import pandas as pd
import numpy as np
from collections import defaultdict
import json

class BiasClean:
    """
    BiasClean v2.0 Main Class

    Implements domain-specific bias scoring and mitigation using evidence-based
    weight matrices derived from UK statistical sources (ONS, NHS, MoJ, FCA, etc.)

    DOMAINS (7):
    - justice, health, finance, education, hiring, business, governance

    FEATURES (7):
    - Ethnicity, SocioeconomicStatus, Region, Age, MigrationStatus, DisabilityStatus, Gender

    USAGE:
    >>> bias_clean = BiasClean()
    >>> bias_clean.fit('health')
    >>> bias_score = bias_clean.score(df)
    >>> df_corrected = bias_clean.transform(df, mode='soft')
    >>> report = bias_clean.report(df, df_corrected)
    """

    def __init__(self):
        """
        Initialize BiasClean v2.0 with UK domain-weight matrix

        Weight matrix based on comprehensive analysis of:
        - ONS (Office for National Statistics)
        - NHS (National Health Service)
        - MoJ (Ministry of Justice)
        - FCA (Financial Conduct Authority)
        - DfE (Department for Education)
        - EHRC (Equality and Human Rights Commission)
        - BEIS (Department for Business, Energy & Industrial Strategy)
        """
        self.weights = self._load_domain_weights()
        self.domain_weights = None
        self.domain = None

    def _load_domain_weights(self):
        """
        Load the final 7×7 weight matrix for UK domains (2025)

        SCHEMA RULE (Option C):
        All 7 features appear in every domain; irrelevant/weaker features
        receive small weights, not removal. Weight vector always sums to 1.00.

        Returns:
            dict: Nested dictionary of domain-specific feature weights
        """
        return {
            'justice': {
                'Ethnicity': 0.25,          # MoJ 'Race & CJS' evidence
                'SocioeconomicStatus': 0.20, # ONS deprivation studies
                'Region': 0.15,              # Regional inequality data
                'Age': 0.15,                 # ONS demographic risk profiles
                'MigrationStatus': 0.10,     # ONS migration access gaps
                'DisabilityStatus': 0.10,    # Equality Act, DWP data
                'Gender': 0.05               # EHRC gender disparities
            },
            'health': {
                'Ethnicity': 0.25,           # NHS ethnicity health gaps
                'SocioeconomicStatus': 0.20, # ONS deprivation-health links
                'DisabilityStatus': 0.15,    # NHS disability access data
                'Gender': 0.15,              # NHS gender treatment variations
                'Region': 0.10,              # Regional health inequality
                'Age': 0.10,                 # ONS age-health gradients
                'MigrationStatus': 0.05      # NHS migrant access data
            },
            'finance': {
                'SocioeconomicStatus': 0.30, # FCA financial exclusion data
                'Region': 0.20,              # Regional economic disparities
                'Ethnicity': 0.20,           # FCA ethnicity lending gaps
                'Age': 0.10,                 # Age-based financial access
                'Gender': 0.10,              # Gender wealth gap data
                'MigrationStatus': 0.05,     # Migrant financial access
                'DisabilityStatus': 0.05     # Disability financial exclusion
            },
            'education': {
                'SocioeconomicStatus': 0.25, # DfE deprivation attainment gaps
                'Ethnicity': 0.20,           # DfE ethnicity attainment data
                'Region': 0.15,              # Regional education inequality
                'DisabilityStatus': 0.15,    # SEND provision disparities
                'Gender': 0.10,              # Gender attainment differences
                'Age': 0.10,                 # Age-stage educational outcomes
                'MigrationStatus': 0.05      # Migrant educational access
            },
            'hiring': {
                'Ethnicity': 0.25,           # EHRC employment discrimination
                'Gender': 0.20,              # ONS gender pay gap, hiring bias
                'DisabilityStatus': 0.15,    # Disability employment gap
                'SocioeconomicStatus': 0.15, # Social mobility commission
                'Region': 0.10,              # Regional employment variations
                'Age': 0.10,                 # Age discrimination in hiring
                'MigrationStatus': 0.05      # Migrant employment access
            },
            'business': {
                'Ethnicity': 0.25,           # BEIS business ownership gaps
                'Gender': 0.20,              # Gender entrepreneurship gaps
                'SocioeconomicStatus': 0.15, # Business access by background
                'Region': 0.15,              # Regional business environment
                'Age': 0.10,                 # Age and business success
                'DisabilityStatus': 0.10,    # Disability business access
                'MigrationStatus': 0.05      # Migrant entrepreneurship
            },
            'governance': {
                'Ethnicity': 0.25,           # Political representation gaps
                'Gender': 0.20,              # Gender representation in governance
                'SocioeconomicStatus': 0.15, # Class background in leadership
                'Region': 0.15,              # Regional representation
                'MigrationStatus': 0.10,     # Migrant civic engagement
                'DisabilityStatus': 0.10,    # Disability representation
                'Age': 0.05                  # Age diversity in governance
            }
        }

    def fit(self, domain: str):
        """
        Set the domain for bias analysis and mitigation

        Args:
            domain (str): One of 'justice', 'health', 'finance', 'education',
                         'hiring', 'business', 'governance'

        Returns:
            self: Returns instance for method chaining

        Raises:
            ValueError: If domain not in predefined list
        """
        if domain not in self.weights:
            raise ValueError(f"Domain must be one of: {list(self.weights.keys())}")
        self.domain = domain
        self.domain_weights = self.weights[domain]
        return self

    def _calculate_disparity(self, df, feature):
        """
        Calculate normalised disparity for a feature using representation ratio

        METHODOLOGY:
        Uses logarithmic representation ratio to measure deviation from
        expected uniform distribution while maintaining scale invariance.

        Args:
            df (pd.DataFrame): Input dataset
            feature (str): Feature column name to analyze

        Returns:
            float: Normalised disparity score (0 = perfect representation)
        """
        if feature not in df.columns:
            return 0.0

        # Calculate observed proportions
        value_counts = df[feature].value_counts(normalize=True)
        expected_proportion = 1.0 / len(value_counts)

        # Calculate average disparity using representation ratio
        total_disparity = 0.0
        for proportion in value_counts:
            rr = proportion / expected_proportion  # Representation Ratio
            disparity = abs(np.log(rr)) if rr > 0 else 1.0  # Log-normalized
            total_disparity += disparity

        return total_disparity / len(value_counts)  # Average disparity

    def score(self, df) -> float:
        """
        Calculate overall bias score for the dataset

        FORMULA:
        BiasScore = Σ (Weight_feature × NormalisedDisparity_feature)

        Args:
            df (pd.DataFrame): Dataset to evaluate

        Returns:
            float: Overall bias score (higher = more biased)

        Raises:
            ValueError: If fit() not called first
        """
        if self.domain_weights is None:
            raise ValueError("Must call fit() with a domain first")

        bias_score = 0.0
        for feature, weight in self.domain_weights.items():
            disparity = self._calculate_disparity(df, feature)
            bias_score += weight * disparity

        return bias_score

    def _downsample(self, df, mode='soft'):
        """
        Apply selective downsampling to reduce over-representation

        ALGORITHM:
        1. Process features by weight priority (highest first)
        2. Identify over-represented groups
        3. Remove records proportionally to weight and excess ratio
        4. Enforce maximum data loss cap (8-15%)

        Args:
            df (pd.DataFrame): Input data to correct
            mode (str): 'soft' (8% max loss) or 'strict' (15% max loss)

        Returns:
            pd.DataFrame: Corrected dataset with reduced bias
        """
        max_removal = 0.15 if mode == 'strict' else 0.08
        original_size = len(df)

        # Process features by weight priority (highest impact first)
        for feature, weight in sorted(self.domain_weights.items(),
                                    key=lambda x: x[1], reverse=True):
            if feature not in df.columns:
                continue

            # Check data loss cap
            current_removal = (original_size - len(df)) / original_size
            if current_removal >= max_removal:
                break

            # Calculate representation disparities
            value_counts = df[feature].value_counts(normalize=True)
            expected = 1.0 / len(value_counts)

            # Downsample over-represented groups
            for value, proportion in value_counts.items():
                if proportion > expected:
                    excess_ratio = (proportion - expected) / proportion
                    group_df = df[df[feature] == value]

                    # Calculate removal count weighted by feature importance
                    n_remove = int(len(group_df) * excess_ratio * weight)

                    # Enforce maximum data loss constraint
                    if len(df) - n_remove < original_size * (1 - max_removal):
                        n_remove = len(df) - int(original_size * (1 - max_removal))

                    # Apply downsampling
                    if n_remove > 0:
                        remove_indices = np.random.choice(group_df.index, n_remove, replace=False)
                        df = df.drop(remove_indices)

        return df

    def transform(self, df, mode='soft'):
        """
        Apply bias mitigation transformation to dataset

        Args:
            df (pd.DataFrame): Input dataset to correct
            mode (str): 'soft' (conservative) or 'strict' (aggressive)

        Returns:
            pd.DataFrame: Bias-corrected dataset

        Raises:
            ValueError: If fit() not called first
        """
        if self.domain_weights is None:
            raise ValueError("Must call fit() with a domain first")

        df_corrected = df.copy()
        df_corrected = self._downsample(df_corrected, mode)
        return df_corrected

    def report(self, df_before, df_after):
        """
        Generate comprehensive fairness diagnostics report

        Args:
            df_before (pd.DataFrame): Original dataset
            df_after (pd.DataFrame): Corrected dataset

        Returns:
            dict: Comprehensive fairness metrics and diagnostics
        """
        score_before = self.score(df_before)
        score_after = self.score(df_after)

        report = {
            'domain': self.domain,
            'bias_score_before': score_before,
            'bias_score_after': score_after,
            'reduction_percent': ((score_before - score_after) / score_before) * 100,
            'records_before': len(df_before),
            'records_after': len(df_after),
            'data_loss_percent': ((len(df_before) - len(df_after)) / len(df_before)) * 100,
            'feature_weights': self.domain_weights,
            'mitigation_efficiency': ((score_before - score_after) /
                                    ((len(df_before) - len(df_after)) / len(df_before)))
                                    if len(df_before) != len(df_after) else 0
        }

        return report

def generate_synthetic_data(domain='health', n_samples=1000):
    """
    Generate synthetic UK dataset for testing and demonstration

    DISTRIBUTIONS BASED ON:
    - ONS 2021 Census data for ethnicity, region, age, gender
    - NHS digital for disability statistics
    - DWP for socioeconomic status
    - ONS for migration patterns

    Args:
        domain (str): Domain to generate data for
        n_samples (int): Number of records to generate

    Returns:
        pd.DataFrame: Synthetic dataset with realistic UK distributions
    """
    np.random.seed(42)  # For reproducible results

    # UK demographic distributions (ONS 2021 Census based)
    data = {
        'Ethnicity': np.random.choice(
            ['White', 'Asian', 'Black', 'Mixed', 'Other'],
            n_samples,
            p=[0.86, 0.08, 0.03, 0.02, 0.01]  # ONS 2021 proportions
        ),
        'Region': np.random.choice(
            ['London', 'South East', 'North West', 'Scotland', 'Wales', 'Other'],
            n_samples,
            p=[0.13, 0.14, 0.11, 0.08, 0.05, 0.49]  # ONS regional distribution
        ),
        'Age': np.random.randint(18, 80, n_samples),  # Working age population
        'Gender': np.random.choice(
            ['Male', 'Female', 'Other'],
            n_samples,
            p=[0.49, 0.50, 0.01]  # ONS gender distribution
        ),
        'DisabilityStatus': np.random.choice(
            ['Yes', 'No'],
            n_samples,
            p=[0.22, 0.78]  # ONS disability prevalence
        ),
        'SocioeconomicStatus': np.random.choice(
            ['High', 'Medium', 'Low'],
            n_samples,
            p=[0.30, 0.50, 0.20]  # Simplified SES distribution
        ),
        'MigrationStatus': np.random.choice(
            ['UK-born', 'Migrant'],
            n_samples,
            p=[0.87, 0.13]  # ONS migration patterns
        ),
        'Outcome': np.random.choice([0, 1], n_samples, p=[0.5, 0.5])  # Binary outcome
    }

    # Domain-specific bias introduction for realistic testing
    if domain == 'justice':
        # Simulate justice system disparities (MoJ evidence)
        data['Outcome'] = np.where(data['Ethnicity'] == 'Black',
                                 np.random.choice([0, 1], n_samples, p=[0.3, 0.7]),
                                 np.random.choice([0, 1], n_samples, p=[0.5, 0.5]))

    return pd.DataFrame(data)

def demonstration():
    """
    Comprehensive demonstration of BiasClean v2.0 capabilities

    Shows single domain testing and multi-domain validation
    """
    print("="*70)
    print("BIASCLEAN V2.0 - OFFICIAL DEMONSTRATION")
    print("Domain-Specific Bias Detection and Mitigation Toolkit")
    print("UK-Focused Fairness Engine")
    print("="*70)

    # Single domain demonstration
    print("\n1. SINGLE DOMAIN TESTING (Health Domain)")
    print("-" * 50)

    domain = 'health'
    print(f"Testing domain: {domain.upper()}")

    # Generate and analyze data
    df = generate_synthetic_data(domain)
    print(f"✓ Generated synthetic dataset: {len(df):,} records")

    # Initialize and fit BiasClean
    bias_clean = BiasClean()
    bias_clean.fit(domain)
    print(f"✓ Loaded domain weights: {domain}")

    # Calculate and display bias scores
    initial_score = bias_clean.score(df)
    print(f"✓ Initial bias score: {initial_score:.4f}")

    # Apply correction
    df_corrected = bias_clean.transform(df, mode='soft')
    report = bias_clean.report(df, df_corrected)

    print(f"✓ Bias score after correction: {report['bias_score_after']:.4f}")
    print(f"✓ Bias reduction: {report['reduction_percent']:.1f}%")
    print(f"✓ Data loss: {report['data_loss_percent']:.1f}%")
    print(f"✓ Final records: {report['records_after']:,}")

    # Multi-domain validation
    print("\n2. MULTI-DOMAIN VALIDATION")
    print("-" * 50)

    domains = ['justice', 'health', 'finance', 'education', 'hiring', 'business', 'governance']
    results = []

    for domain in domains:
        df = generate_synthetic_data(domain, n_samples=1500)
        bias_clean = BiasClean().fit(domain)

        initial_score = bias_clean.score(df)
        df_corrected = bias_clean.transform(df, mode='soft')
        final_score = bias_clean.score(df_corrected)

        results.append({
            'domain': domain,
            'initial_score': initial_score,
            'final_score': final_score,
            'reduction_percent': ((initial_score - final_score) / initial_score) * 100,
            'records_after': len(df_corrected),
            'data_loss_percent': ((1500 - len(df_corrected)) / 1500) * 100
        })

    # Display comprehensive results
    results_df = pd.DataFrame(results)
    print("\nMulti-Domain Performance Summary:")
    print("=" * 70)
    print(results_df.round(4).to_string(index=False))

    # Summary statistics
    avg_reduction = results_df['reduction_percent'].mean()
    avg_data_loss = results_df['data_loss_percent'].mean()

    print(f"\nSUMMARY STATISTICS:")
    print(f"Average Bias Reduction: {avg_reduction:.2f}%")
    print(f"Average Data Loss: {avg_data_loss:.2f}%")
    print(f"Most Biased Domain: {results_df.loc[results_df['initial_score'].idxmax(), 'domain']}")
    print(f"Least Biased Domain: {results_df.loc[results_df['initial_score'].idxmin(), 'domain']}")

    print("\n" + "="*70)
    print("BIASCLEAN V2.0 VALIDATION COMPLETE")
    print("Pipeline ready for production deployment")
    print("="*70)

if __name__ == "__main__":
    """
    Main execution block for demonstration and testing

    Run this file directly to see BiasClean v2.0 in action
    """
    demonstration()