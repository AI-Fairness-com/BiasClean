# -*- coding: utf-8 -*-
"""Dual_BiasClean.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vwpuDwQCUlA1ZKFmlVOF3RcRr7Gg2jEx
"""

# -*- coding: utf-8 -*-
"""Optimised_BiasClean.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12dZTHt0srZJ0TaVQ9iezOY-ZZ5U1vIT6
"""

# -*- coding: utf-8 -*-
"""
Universal BiasClean Pipeline v2.1
Weight-Prioritized Bias Mitigation for Justice Domains

Author: [Your Name]
Date: December 2025
License: [Your License]
"""

# ============================================================================
# IMPORTS
# ============================================================================

import pandas as pd
import numpy as np
import json
import os
import warnings
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
from scipy.stats import fisher_exact, chi2_contingency
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display, HTML, Markdown

warnings.filterwarnings('ignore')

# ============================================================================
# DOMAIN CONFIGURATIONS
# ============================================================================

DOMAIN_CONFIGS = {
    "justice": {
        "weights": {
            "Ethnicity": 0.25,
            "SocioeconomicStatus": 0.20,
            "Region": 0.15,
            "Age": 0.15,
            "MigrationStatus": 0.10,
            "DisabilityStatus": 0.10,
            "Gender": 0.05
        },
        "outcome_patterns": ["recid", "reoffend", "rearrest", "violat", "outcome", "target", "label"],
        "ontology_extensions": {},
        "report_labels": {
            "outcome": "Recidivism Rate",
            "title": "Justice Bias Analysis Report",
            "domain": "Justice"
        }
    },
    "health": {
        "weights": {
            "Ethnicity": 0.25,
            "SocioeconomicStatus": 0.20,
            "Region": 0.10,
            "Age": 0.10,
            "MigrationStatus": 0.05,
            "DisabilityStatus": 0.15,
            "Gender": 0.15
        },
        "outcome_patterns": ["diagnosis", "mortality", "readmission", "stage", "outcome", "target", "triage", "screening", "positive", "negative", "case", "control"],
        "ontology_extensions": {
            "ClinicalFactors": {
                "patterns": ["bmi", "smoking", "history", "medication", "treatment", "therapy", "symptom", "comorbid"],
                "data_types": ["object", "int64", "float64"]
            }
        },
        "report_labels": {
            "outcome": "Diagnosis Rate",
            "title": "Healthcare Bias Analysis Report",
            "domain": "Health"
        }
    }
}

# ============================================================================
# 1. HIERARCHICAL MAPPER
# ============================================================================

class HierarchicalMapper:
    """3-tier hierarchical feature mapper: Universal → Domain → Jurisdiction"""

    def __init__(self, domain="justice"):
        self.domain = domain
        self.config = DOMAIN_CONFIGS.get(domain, DOMAIN_CONFIGS["justice"])

        # Universal ontology (highest priority)
        self.universal_ontology = {
            "Age": {
                "patterns": ["age", "birth_year", "dob", "date_of_birth", "age_at", "years_old"],
                "data_types": ["int64", "float64"]
            },
            "Ethnicity": {
                "patterns": ["race", "ethnicity", "ethnic", "racial"],
                "value_mappings": {
                    "African-American": ["black", "african american", "aa", "afr_am"],
                    "Caucasian": ["white", "caucasian", "european"],
                    "Hispanic": ["latino", "latina", "latinx", "hispanic"],
                    "Asian": ["asian", "pacific islander", "api"],
                    "Native American": ["native", "indigenous", "american indian"],
                    "Other": ["other", "mixed", "multiracial"]
                }
            },
            "Gender": {
                "patterns": ["sex", "gender"],
                "value_mappings": {
                    "Male": ["m", "male", "man", "men"],
                    "Female": ["f", "female", "woman", "women"]
                }
            }
        }

        # Justice domain ontology
        self.justice_ontology = {
            "outcome_patterns": self.config["outcome_patterns"],
            "contextual_patterns": {
                "SocioeconomicStatus": ["prior", "count", "criminal_history", "arrest", "convict"],
                "Region": ["charge", "offense", "county", "jurisdiction", "district"]
            },
            "exclusion_patterns": ["id", "name", "case", "number", "docket", "person_id", "defendant_id"]
        }

    def map_column(self, column_name: str, sample_values: pd.Series) -> Dict:
        """Map a single column using 3-tier hierarchical rules"""
        col_lower = column_name.lower()

        # TIER 1: Universal attributes (highest priority)
        for feature, config in self.universal_ontology.items():
            for pattern in config["patterns"]:
                if pattern in col_lower:
                    # Validate with sample values
                    value_score = self._validate_values(sample_values, config)
                    confidence = 0.85 + (value_score * 0.1)

                    return {
                        "feature": feature,
                        "confidence": min(confidence, 0.98),
                        "tier": "universal",
                        "justification": f"Column name '{column_name}' matches universal {feature} pattern",
                        "pattern_matched": pattern
                    }

        # TIER 2: Domain-specific (outcome detection)
        for pattern in self.justice_ontology["outcome_patterns"]:
            if pattern in col_lower:
                if sample_values.nunique() == 2:  # Binary outcome
                    return {
                        "feature": "__outcome__",
                        "confidence": 0.90,
                        "tier": "domain",
                        "justification": f"Binary outcome variable with pattern '{pattern}'",
                        "is_outcome": True
                    }
                else:
                    return {
                        "feature": "__outcome__",
                        "confidence": 0.70,
                        "tier": "domain",
                        "justification": f"Potential outcome variable (requires binarization)",
                        "requires_binarization": True
                    }

        # TIER 2: Contextual features
        for feature_type, patterns in self.justice_ontology["contextual_patterns"].items():
            for pattern in patterns:
                if pattern in col_lower:
                    return {
                        "feature": feature_type,
                        "confidence": 0.75,
                        "tier": "domain",
                        "justification": f"Matches {feature_type} pattern '{pattern}'"
                    }

        # Exclusion patterns
        for pattern in self.justice_ontology["exclusion_patterns"]:
            if pattern in col_lower:
                return {
                    "feature": "__exclude__",
                    "confidence": 0.85,
                    "tier": "exclusion",
                    "justification": f"Identifier/administrative column (pattern: '{pattern}')"
                }

        # No match found
        return {
            "feature": None,
            "confidence": 0.20,
            "tier": "unknown",
            "justification": "No semantic pattern match found - requires manual review"
        }

    def _validate_values(self, sample_values: pd.Series, config: Dict) -> float:
        """Validate sample values against expected patterns"""
        if "value_mappings" in config:
            expected_values = set()
            for canonical, aliases in config["value_mappings"].items():
                expected_values.update([v.lower() for v in aliases])

            sample_set = set([str(v).lower() for v in sample_values.dropna().head(50)])
            if sample_set:
                overlap = len(sample_set & expected_values) / len(sample_set)
                return overlap

        if "data_types" in config and pd.api.types.is_numeric_dtype(sample_values):
            return 1.0

        return 0.5

    def map_dataset(self, df: pd.DataFrame) -> Dict[str, Dict]:
        """Map all columns in dataset"""
        mappings = {}
        for column in df.columns:
            sample = df[column].dropna()
            mappings[column] = self.map_column(column, sample)
        return mappings

# ============================================================================
# 2. CONSTRAINT VALIDATOR
# ============================================================================

class ConstraintValidator:
    """Statistical constraint validation for reliable fairness analysis"""

    def __init__(self):
        self.min_samples_per_group = 50
        self.min_total_records = 500
        self.max_missing_rate = 0.3

    def validate_outcome(self, df: pd.DataFrame, column: str) -> Dict:
        """Validate outcome variable suitability"""
        result = {
            "valid": True,
            "warnings": [],
            "errors": [],
            "statistics": {}
        }

        if column not in df.columns:
            result["valid"] = False
            result["errors"].append(f"Column '{column}' not found in dataset")
            return result

        clean_series = df[column].dropna()
        unique_count = clean_series.nunique()

        result["statistics"]["unique_values"] = unique_count
        result["statistics"]["total_samples"] = len(clean_series)

        # Must be binary
        if unique_count != 2:
            result["warnings"].append(f"Outcome has {unique_count} unique values (expected: 2)")
            if unique_count > 5:
                result["valid"] = False
                result["errors"].append("Too many unique values for binary outcome")

        # Check balance
        if unique_count == 2:
            positive_rate = clean_series.mean()
            result["statistics"]["positive_rate"] = positive_rate

            if not (0.05 <= positive_rate <= 0.95):
                result["warnings"].append(f"Class imbalance: {positive_rate:.1%} positive rate")

        # Check missing values
        missing_rate = df[column].isna().mean()
        result["statistics"]["missing_rate"] = missing_rate

        if missing_rate > 0.1:
            result["warnings"].append(f"High missing rate: {missing_rate:.1%}")

        return result

    def validate_protected_attribute(self, df: pd.DataFrame, column: str,
                                     target: str, feature_type: str = None) -> Dict:
        """Validate protected attribute for fairness analysis"""
        result = {
            "valid": True,
            "warnings": [],
            "errors": [],
            "statistics": {}
        }

        if column not in df.columns or target not in df.columns:
            result["valid"] = False
            result["errors"].append("Required columns not found")
            return result

        clean_data = df[[column, target]].dropna()

        # Check sample size per group
        value_counts = clean_data[column].value_counts()
        min_samples = value_counts.min()
        n_groups = len(value_counts)

        result["statistics"]["min_group_size"] = min_samples
        result["statistics"]["n_groups"] = n_groups

        if min_samples < self.min_samples_per_group:
            result["warnings"].append(
                f"Small group size: {min_samples} samples (recommended: {self.min_samples_per_group}+)"
            )

        # Check statistical relationship with outcome
        try:
            contingency = pd.crosstab(clean_data[column], clean_data[target])

            if contingency.shape == (2, 2):
                _, p_value = fisher_exact(contingency)
                test_used = "fisher-exact"
            else:
                chi2, p_value, _, _ = chi2_contingency(contingency)
                test_used = "chi-square"

            result["statistics"]["p_value"] = p_value
            result["statistics"]["test_used"] = test_used

            if p_value >= 0.05:
                result["warnings"].append(
                    f"Weak statistical relationship with outcome (p={p_value:.4f})"
                )

        except Exception as e:
            result["warnings"].append(f"Statistical test failed: {str(e)}")

        return result

# ============================================================================
# 3. USER CONFIRMATION UI (AUTO-APPROVAL MODE)
# ============================================================================
class MappingConfirmationUI:
    """CLI interface for mapping approval with auto-approval capability"""

    def __init__(self):
        self.approved_mappings = {}
        self.rejected_mappings = []

    def auto_approve_high_confidence(self, proposals: Dict,
                                    threshold: float = 0.80) -> Dict:
        """Auto-approve high-confidence mappings for production"""
        print(f"\n{'='*80}")
        print(f"AUTO-APPROVAL MODE (Confidence Threshold: {threshold:.0%})")
        print(f"{'='*80}\n")

        approved_count = 0
        rejected_count = 0

        for column, candidates in proposals.items():
            if not candidates:
                self.rejected_mappings.append(column)
                rejected_count += 1
                continue

            best = max(candidates, key=lambda x: x["confidence"])
            validation = best.get("validation", {})
            confidence = best["confidence"]
            feature = best["feature"]

            # Auto-approve criteria
            should_approve = (
                confidence >= threshold and
                validation.get("valid", True) and
                feature not in [None, "__exclude__"] and
                len(validation.get("errors", [])) == 0
            )

            if should_approve:
                self.approved_mappings[column] = {
                    **best,
                    "auto_approved": True,
                    "threshold_used": threshold
                }

                icon = "✅"
                print(f"{icon} {column:25} → {feature:20} ({confidence:.0%})")
                approved_count += 1
            else:
                self.rejected_mappings.append(column)

# Determine reason
                reasons = []
                if confidence < threshold:
                    reasons.append(f"low confidence ({confidence:.0%})")
                if not validation.get("valid", True):
                    reasons.append("validation failed")
                if feature in [None, "__exclude__"]:
                    reasons.append("excluded/unknown")
                if validation.get("errors"):
                    reasons.append("has errors")

                icon = "⏭️"
                print(f"{icon} {column:25}   SKIPPED ({', '.join(reasons)})")
                rejected_count += 1

        print(f"\n{'='*80}")
        print(f"MAPPING SUMMARY")
        print(f"{'='*80}")
        print(f"✅ Approved: {approved_count} columns")
        print(f"⏭️  Skipped:  {rejected_count} columns")
        print(f"{'='*80}\n")

        return self.approved_mappings

# ============================================================================
# 4. BIASCLEAN ENGINE (Core from COMPAS - Zero Modifications)
# ============================================================================

class BiasCleanEngine:
    """Core bias detection and mitigation engine"""

    def __init__(self, domain_weights: Dict[str, float]):
        self.domain_weights = domain_weights
        self._feature_map = {}
        self._target_column = None

    def score(self, df: pd.DataFrame, target_column: str) -> float:
        """Calculate weighted bias score (0-1, lower is better)"""
        total_score = 0.0
        meaningful_features = 0

        for feature, weight in self.domain_weights.items():
            disparity = self._calculate_disparity(df, feature, target_column)
            if disparity > 0:
                total_score += weight * disparity
                meaningful_features += 1

        if meaningful_features > 0:
            normalization = len(self.domain_weights) / meaningful_features
            total_score *= normalization

        return total_score

    def _calculate_disparity(self, df: pd.DataFrame, feature: str,
                           target_column: str) -> float:
        """Calculate outcome disparity for a protected feature"""
        column = self._feature_map.get(feature)
        if not column or column not in df.columns or target_column not in df.columns:
            return 0.0

        try:
            group_rates = {}
            for group in df[column].unique():
                if pd.isna(group):
                    continue
                group_data = df[df[column] == group]
                if len(group_data) >= 10:
                    rate = group_data[target_column].mean()
                    group_rates[str(group)] = rate

            if len(group_rates) < 2:
                return 0.0

            rates = np.array(list(group_rates.values()))
            mean_rate = np.mean(rates)

            if mean_rate == 0:
                return 0.0

            cv_disparity = np.std(rates) / mean_rate
            return min(cv_disparity, 1.0)

        except Exception:
            return 0.0

    def transform_industry(self, df: pd.DataFrame,
                         diagnostic_results: Dict) -> pd.DataFrame:
        """Apply bias mitigation with weight-prioritized rebalancing"""
        if not diagnostic_results.get("requires_mitigation", False):
            return df.copy()

        df_optimized = df.copy()
        target = self._target_column or diagnostic_results.get("target_column_used")

        # Get features with significant bias, sorted by weight (highest first)
        biased_features = []
        for feature, test_result in diagnostic_results.get("feature_tests", {}).items():
            if test_result.get("significant_bias", False):
                weight = self.domain_weights.get(feature, 0.05)
                biased_features.append((feature, weight))

        # Sort by weight descending (highest weight = highest priority)
        biased_features.sort(key=lambda x: x[1], reverse=True)

        # Apply rebalancing in weight-priority order
        for feature, weight in biased_features:
            column = self._feature_map.get(feature)
            if column and column in df_optimized.columns:
                df_optimized = self._rebalance_feature_weighted(
                    df_optimized, column, target, weight
                )

        return df_optimized

    def _rebalance_feature_weighted(self, df: pd.DataFrame, column: str,
                                   target: str, weight: float) -> pd.DataFrame:
        """Rebalance a specific feature with weight-dependent intensity"""
        df_balanced = df.copy()
        group_stats = df.groupby(column)[target].agg(["mean", "count"])
        overall_mean = df[target].mean()

        # Weight-dependent thresholds (higher weight = more aggressive)
        # Base thresholds scaled by weight (normalized to 0.25 max weight)
        weight_factor = weight / 0.25  # 0.25 is max weight (Ethnicity)

        # Disparity threshold: higher weight = lower threshold (catch more cases)
        disparity_threshold = 0.08 * (1.5 - weight_factor)  # Range: 0.04 to 0.12

        # Removal rate: higher weight = more removal
        removal_rate = 0.05 * (1.0 + weight_factor)  # Range: 0.05 to 0.10

        # Addition rate: higher weight = more addition
        addition_rate = 0.08 * (1.0 + weight_factor)  # Range: 0.08 to 0.16

        # Minimum group size: higher weight = lower threshold
        min_group_size = int(30 * (1.0 - weight_factor * 0.5))  # Range: 15 to 30

        for group, stats in group_stats.iterrows():
            group_mean = stats["mean"]
            group_size = stats["count"]

            # Use weight-adjusted thresholds
            if abs(group_mean - overall_mean) > disparity_threshold and group_size > min_group_size:
                group_mask = df[column] == group

                if group_mean > overall_mean:  # High recidivism group
                    positives = df[group_mask & (df[target] == 1)]
                    if len(positives) > 10:
                        # Weight-dependent removal
                        remove_n = min(len(positives) // 10, int(group_size * removal_rate))
                        if remove_n > 0:
                            remove_idx = positives.sample(n=remove_n, random_state=42).index
                            df_balanced = df_balanced.drop(remove_idx)

                else:  # Low recidivism group
                    positives = df[group_mask & (df[target] == 1)]
                    if len(positives) > 5:
                        # Weight-dependent addition
                        add_n = min(len(positives) // 5, int(group_size * addition_rate))
                        if add_n > 0:
                            add_samples = positives.sample(n=add_n, replace=True, random_state=42)
                            df_balanced = pd.concat([df_balanced, add_samples], ignore_index=True)

        return df_balanced

    # Keep original method for backward compatibility
    def _rebalance_feature(self, df: pd.DataFrame, column: str,
                          target: str) -> pd.DataFrame:
        """Original rebalancing (uses default weight = 0.05)"""
        return self._rebalance_feature_weighted(df, column, target, weight=0.05)

    def validate_industry_readiness(self, df_before: pd.DataFrame,
                                   df_after: pd.DataFrame,
                                   diagnostic_results: Dict) -> Dict:
        """Validate mitigation results"""
        validation = {
            "fairness_improvement": {},
            "data_integrity": {
                "records_before": len(df_before),
                "records_after": len(df_after),
                "retention_rate": (len(df_after) / len(df_before)) * 100
            }
        }

        target = diagnostic_results.get("target_column_used")

        for feature in diagnostic_results.get("feature_tests", {}).keys():
            before = self._calculate_disparity(df_before, feature, target)
            after = self._calculate_disparity(df_after, feature, target)

            if before > 0:
                improvement = ((before - after) / before) * 100
                validation["fairness_improvement"][feature] = improvement

        return validation

# ============================================================================
# 5. VISUALIZATION ENGINE
# ============================================================================

class VisualizationEngine:
    """Generate comprehensive visualizations for bias analysis"""

    def __init__(self):
        sns.set_style("whitegrid")
        plt.rcParams['figure.figsize'] = (12, 6)

    def plot_disparity_comparison(self, original_df: pd.DataFrame,
                                  corrected_df: pd.DataFrame,
                                  feature_map: Dict, target: str,
                                  save_path: str = None):
        """Plot before/after disparity comparison"""
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))

        for idx, (df, title) in enumerate([(original_df, "Before Mitigation"),
                                           (corrected_df, "After Mitigation")]):
            ax = axes[idx]

            # Plot recidivism rates by protected attributes
            for feature, column in feature_map.items():
                if column in df.columns and feature not in ["SocioeconomicStatus", "Region"]:
                    rates = df.groupby(column)[target].mean()
                    rates.plot(kind='bar', ax=ax, alpha=0.7, label=feature)

            ax.set_title(title, fontsize=14, fontweight='bold')
            ax.set_ylabel("Recidivism Rate", fontsize=12)
            ax.set_xlabel("Groups", fontsize=12)
            ax.legend(loc='upper right')
            ax.axhline(y=df[target].mean(), color='r', linestyle='--',
                      label='Overall Mean', alpha=0.5)
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')

        plt.show()

    def plot_feature_improvements(self, validation_results: Dict,
                                  save_path: str = None):
        """Plot fairness improvements by feature"""
        improvements = validation_results.get("fairness_improvement", {})

        if not improvements:
            print("No fairness improvements to visualize")
            return

        fig, ax = plt.subplots(figsize=(10, 6))

        features = list(improvements.keys())
        values = list(improvements.values())

        colors = ['green' if v > 0 else 'red' for v in values]

        ax.barh(features, values, color=colors, alpha=0.7)
        ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)
        ax.set_xlabel('Improvement (%)', fontsize=12)
        ax.set_title('Fairness Improvements by Protected Feature',
                    fontsize=14, fontweight='bold')
        ax.grid(axis='x', alpha=0.3)

        # Add value labels
        for i, v in enumerate(values):
            ax.text(v + (2 if v > 0 else -2), i, f'{v:+.1f}%',
                   va='center', ha='left' if v > 0 else 'right')

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')

        plt.show()

    def plot_data_integrity(self, validation_results: Dict,
                           save_path: str = None):
        """Plot data integrity metrics"""
        integrity = validation_results.get("data_integrity", {})

        fig, ax = plt.subplots(figsize=(8, 6))

        before = integrity.get("records_before", 0)
        after = integrity.get("records_after", 0)
        retention = integrity.get("retention_rate", 100)

        ax.bar(['Before Mitigation', 'After Mitigation'],
               [before, after], color=['#3498db', '#2ecc71'], alpha=0.7)

        ax.set_ylabel('Number of Records', fontsize=12)
        ax.set_title('Data Integrity: Record Count',
                    fontsize=14, fontweight='bold')

        # Add retention rate annotation
        ax.text(0.5, max(before, after) * 0.95,
               f'Retention Rate: {retention:.1f}%',
               ha='center', fontsize=12,
               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

        # Add value labels
        for i, v in enumerate([before, after]):
            ax.text(i, v + (max(before, after) * 0.02), f'{v:,}',
                   ha='center', va='bottom', fontsize=11)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')

        plt.show()

# ============================================================================
# 6. REPORT GENERATOR
# ============================================================================

class ReportGenerator:
    """Generate comprehensive human-readable reports"""

    def generate_html_report(self, results: Dict, output_file: str = "biasclean_report.html"):
        """Generate comprehensive HTML report"""

        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>BiasClean Analysis title>BiasClean Analysis Report - {self.config["report_labels"]["domain"]}</title>Report</title>
            <style>
                body {{
                    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                    max-width: 1200px;
                    margin: 0 auto;
                    padding: 20px;
                    background-color: #f5f5f5;
                }}
                .header {{
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white;
                    padding: 30px;
                    border-radius: 10px;
                    margin-bottom: 30px;
                }}
                .section {{
                    background: white;
                    padding: 25px;
                    margin-bottom: 20px;
                    border-radius: 8px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }}
                .metric {{
                    display: inline-block;
                    background: #f8f9fa;
                    padding: 15px 25px;
                    margin: 10px;
                    border-radius: 5px;
                    border-left: 4px solid #667eea;
                }}
                .metric-value {{
                    font-size: 28px;
                    font-weight: bold;
                    color: #667eea;
                }}
                .metric-label {{
                    font-size: 14px;
                    color: #666;
                    margin-top: 5px;
                }}
                .success {{
                    color: #27ae60;
                    font-weight: bold;
                }}
                .warning {{
                    color: #f39c12;
                    font-weight: bold;
                }}
                .error {{
                    color: #e74c3c;
                    font-weight: bold;
                }}
                table {{
                    width: 100%;
                    border-collapse: collapse;
                    margin-top: 15px;
                }}
                th, td {{
                    padding: 12px;
                    text-align: left;
                    border-bottom: 1px solid #ddd;
                }}
                th {{
                    background-color: #667eea;
                    color: white;
                }}
                tr:hover {{
                    background-color: #f5f5f5;
                }}
                .footer {{
                    text-align: center;
                    color: #666;
                    margin-top: 40px;
                    padding: 20px;
                }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>{self.config["report_labels"]["title"]}</h1>
                <p>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                <p>Domain: {self.config["report_labels"]["domain"]} | Version: 2.0</p>
            </div>
        """

# Executive Summary
        diagnostics = results.get("diagnostics", {})
        validation = results.get("validation", {})

        initial_bias = diagnostics.get("initial_bias_score", 0)
        final_bias = diagnostics.get("final_bias_score", initial_bias)
        improvement = ((initial_bias - final_bias) / initial_bias * 100) if initial_bias > 0 else 0

        html_content += f"""
            <div class="section">
                <h2>Executive Summary</h2>
                <div class="metric">
                    <div class="metric-value">{initial_bias:.3f} → {final_bias:.3f}</div>
                    <div class="metric-label">Bias Score</div>
                </div>
                <div class="metric">
                    <div class="metric-value {'success' if improvement > 0 else 'error'}">{improvement:+.1f}%</div>
                    <div class="metric-label">Overall Improvement</div>
                </div>
                <div class="metric">
                    <div class="metric-value">{diagnostics.get('significant_bias_count', 0)}</div>
                    <div class="metric-label">Significant Biases</div>
                </div>
                <div class="metric">
                    <div class="metric-value">{validation.get('data_integrity', {}).get('retention_rate', 100):.1f}%</div>
                    <div class="metric-label">Data Retention</div>
                </div>
            </div>
        """

        # Feature Mappings
        mappings = results.get("mappings", {})
        html_content += """
            <div class="section">
                <h2>Feature Mappings</h2>
                <table>
                    <tr>
                        <th>Original Column</th>
                        <th>Mapped Feature</th>
                        <th>Confidence</th>
                        <th>Tier</th>
                    </tr>
        """

        for col, mapping in mappings.items():
            feature = mapping.get("feature", "Unknown")
            confidence = mapping.get("confidence", 0) * 100
            tier = mapping.get("tier", "unknown")

            html_content += f"""
                    <tr>
                        <td>{col}</td>
                        <td><strong>{feature}</strong></td>
                        <td>{confidence:.0f}%</td>
                        <td><span class="{'success' if tier=='universal' else 'warning'}">{tier}</span></td>
                    </tr>
            """

        html_content += """
                </table>
            </div>
        """

        # Fairness Improvements
        improvements = validation.get("fairness_improvement", {})
        if improvements:
            html_content += """
                <div class="section">
                    <h2>Fairness Improvements by Feature</h2>
                    <table>
                        <tr>
                            <th>Protected Feature</th>
                            <th>Improvement</th>
                            <th>Status</th>
                        </tr>
            """

            for feature, imp_value in improvements.items():
                status = "Improved" if imp_value > 0 else "Worsened"
                status_class = "success" if imp_value > 0 else "error"

                html_content += f"""
                        <tr>
                            <td><strong>{feature}</strong></td>
                            <td><span class="{status_class}">{imp_value:+.1f}%</span></td>
                            <td>{status}</td>
                        </tr>
                """

            html_content += """
                    </table>
                </div>
            """

        # Statistical Tests
        feature_tests = diagnostics.get("feature_tests", {})
        if feature_tests:
            html_content += """
                <div class="section">
                    <h2>Statistical Tests</h2>
                    <table>
                        <tr>
                            <th>Feature</th>
                            <th>Test Used</th>
                            <th>P-Value</th>
                            <th>Significant Bias</th>
                        </tr>
            """

            for feature, test in feature_tests.items():
                p_val = test.get("p_value", 1.0)
                sig = test.get("significant_bias", False)
                test_used = test.get("test_used", "N/A")

                html_content += f"""
                        <tr>
                            <td><strong>{feature}</strong></td>
                            <td>{test_used}</td>
                            <td>{p_val:.6f}</td>
                            <td><span class="{'error' if sig else 'success'}">{'Yes' if sig else 'No'}</span></td>
                        </tr>
                """

            html_content += """
                </table>
            </div>
        """

        # Footer
        html_content += """
            <div class="footer">
                <p>Generated by Universal BiasClean v2.0</p>
                <p>Hierarchical Taxonomy + Constraint Validation Framework</p>
            </div>
        </body>
        </html>
        """

        # Save to file
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html_content)

        print(f"HTML report saved to: {output_file}")
        return output_file

    def print_console_report(self, results: Dict):
        """Print comprehensive console report"""
        print("\n" + "="*80)
        print("BIASCLEAN ANALYSIS REPORT")
        print("="*80)

        diagnostics = results.get("diagnostics", {})
        validation = results.get("validation", {})

        # Executive Summary
        print("\nEXECUTIVE SUMMARY")
        print("-"*80)

        initial_bias = diagnostics.get("initial_bias_score", 0)
        final_bias = diagnostics.get("final_bias_score", initial_bias)
        improvement = ((initial_bias - final_bias) / initial_bias * 100) if initial_bias > 0 else 0

        print(f"Initial Bias Score:     {initial_bias:.4f}")
        print(f"Final Bias Score:       {final_bias:.4f}")
        print(f"Overall Improvement:    {improvement:+.1f}%")
        print(f"Significant Biases:     {diagnostics.get('significant_bias_count', 0)}")

        # Data Integrity
        integrity = validation.get("data_integrity", {})
        print(f"\nRecords Before:         {integrity.get('records_before', 0):,}")
        print(f"Records After:          {integrity.get('records_after', 0):,}")
        print(f"Retention Rate:         {integrity.get('retention_rate', 100):.1f}%")

        # Feature Improvements
        improvements = validation.get("fairness_improvement", {})
        if improvements:
            print("\nFAIRNESS IMPROVEMENTS BY FEATURE")
            print("-"*80)
            for feature, imp_value in improvements.items():
                icon = "✅" if imp_value > 0 else "⚠️"
                print(f"{icon} {feature:25} {imp_value:+.1f}%")

        # Mappings Summary
        mappings = results.get("mappings", {})
        print(f"\nFEATURE MAPPINGS")
        print("-"*80)
        print(f"Total Approved:         {len(mappings)}")

        tier_counts = {}
        for mapping in mappings.values():
            tier = mapping.get("tier", "unknown")
            tier_counts[tier] = tier_counts.get(tier, 0) + 1

        for tier, count in tier_counts.items():
            print(f"  • {tier.title():15}  {count}")

        print("\n" + "="*80)

# ============================================================================
# 7. UNIVERSAL BIASCLEAN PIPELINE (Main Orchestrator)
# ============================================================================

class UniversalBiasClean:
    """Main orchestration pipeline for Universal BiasClean"""

    def __init__(self, domain: str = "justice", jurisdiction: Optional[str] = None):
        self.domain = domain
        self.jurisdiction = jurisdiction
        self.config = DOMAIN_CONFIGS.get(domain, DOMAIN_CONFIGS["justice"])

        # Initialize components
        self.mapper = HierarchicalMapper(domain=domain)
        self.validator = ConstraintValidator()
        self.ui = MappingConfirmationUI()
        self.viz = VisualizationEngine()
        self.reporter = ReportGenerator()

    # Domain-specific weights
        self.engine = BiasCleanEngine(self.config["weights"])

        # State variables
        self.original_df = None
        self.corrected_df = None
        self.approved_mappings = {}
        self.results = {}

        print(f"\n{'='*80}")
        print(f"UNIVERSAL BIASCLEAN v2.0 - INITIALIZED")
        print(f"{'='*80}")
        print(f"   Domain:        {domain.upper()}")
        print(f"   Jurisdiction:  {jurisdiction or 'Default'}")
        print(f"   Framework:     Hierarchical Taxonomy + Constraint Validation")
        print(f"{'='*80}\n")

    def process_dataset(self, file_path: str = None, df: pd.DataFrame = None,
                       target_column: Optional[str] = None,
                       auto_approve_threshold: float = 0.80) -> Dict[str, Any]:
        """
        Complete pipeline execution

        Args:
            file_path: Path to CSV file (optional if df provided)
            df: DataFrame to process (optional if file_path provided)
            target_column: Outcome variable column name (optional, auto-detected)
            auto_approve_threshold: Confidence threshold for auto-approval

        Returns:
            Complete results dictionary
        """
        try:
            # ================================================================
            # PHASE 1: DATASET LOADING
            # ================================================================
            print(f"\n{'='*80}")
            print("PHASE 1: DATASET LOADING")
            print(f"{'='*80}")

            if df is not None:
                self.original_df = df.copy()
                print(f"Loaded DataFrame from memory")
            elif file_path:
                self.original_df = pd.read_csv(file_path)
                print(f"Loaded CSV: {file_path}")
            else:
                raise ValueError("Must provide either file_path or df")

            print(f"   Records:  {len(self.original_df):,}")
            print(f"   Columns:  {len(self.original_df.columns)}")
            print(f"   Memory:   {self.original_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

            # ================================================================
            # PHASE 2: HIERARCHICAL MAPPING
            # ================================================================
            print(f"\n{'='*80}")
            print("PHASE 2: HIERARCHICAL FEATURE MAPPING")
            print(f"{'='*80}")

            raw_mappings = self.mapper.map_dataset(self.original_df)

            # Count by tier
            tier_counts = {}
            for mapping in raw_mappings.values():
                tier = mapping.get("tier", "unknown")
                tier_counts[tier] = tier_counts.get(tier, 0) + 1

            print(f"   Analyzed {len(raw_mappings)} columns:")
            for tier, count in sorted(tier_counts.items()):
                print(f"     • {tier.title()}: {count}")

            # ================================================================
            # PHASE 3: CONSTRAINT VALIDATION
            # ================================================================
            print(f"\n{'='*80}")
            print("PHASE 3: CONSTRAINT VALIDATION")
            print(f"{'='*80}")

            # Determine target column
            if target_column and target_column in self.original_df.columns:
                final_target = target_column
                print(f"   Using specified target: '{final_target}'")
            else:
                # Auto-detect outcome column
                outcome_candidates = []
                for col, mapping in raw_mappings.items():
                    if mapping.get("feature") == "__outcome__":
                        outcome_candidates.append((col, mapping.get("confidence", 0)))

                if outcome_candidates:
                    # Sort by confidence and take best
                    outcome_candidates.sort(key=lambda x: x[1], reverse=True)
                    final_target = outcome_candidates[0][0]
                    print(f"   Auto-detected target: '{final_target}' (confidence: {outcome_candidates[0][1]:.0%})")
                else:
                    # Find any binary column
                    for col in self.original_df.columns:
                        if self.original_df[col].nunique() == 2:
                            final_target = col
                            print(f"   Using binary column as target: '{final_target}'")
                            break
                    else:
                        raise ValueError("No suitable target column found. Please specify manually.")

            # Validate outcome
            outcome_validation = self.validator.validate_outcome(self.original_df, final_target)

            if outcome_validation.get("errors"):
                print(f"   Outcome validation errors:")
                for error in outcome_validation["errors"]:
                    print(f"      • {error}")

            if outcome_validation.get("warnings"):
                print(f"   Outcome validation warnings:")
                for warning in outcome_validation["warnings"][:3]:  # Show first 3
                    print(f"      • {warning}")

            # Validate all mappings
            validated_proposals = {}
            validation_summary = {"passed": 0, "warnings": 0, "errors": 0}

            for column, mapping in raw_mappings.items():
                if column == final_target:
                    validated_proposals[column] = [{
                        **mapping,
                        "feature": "__outcome__",
                        "validation": outcome_validation
                    }]
                else:
                    feature = mapping.get("feature")
                    if feature and feature not in ["__exclude__", None]:
                        validation = self.validator.validate_protected_attribute(
                            self.original_df, column, final_target, feature
                        )

                        if validation.get("errors"):
                            validation_summary["errors"] += 1
                        elif validation.get("warnings"):
                            validation_summary["warnings"] += 1
                        else:
                            validation_summary["passed"] += 1

                        validated_proposals[column] = [{
                            **mapping,
                            "validation": validation
                        }]
                    else:
                        validated_proposals[column] = [mapping]

            print(f"\n   Validation Summary:")
            print(f"     Passed:   {validation_summary['passed']}")
            print(f"     Warnings: {validation_summary['warnings']}")
            print(f"     Errors:   {validation_summary['errors']}")

            # ================================================================
            # PHASE 4: MAPPING CONFIRMATION (AUTO-APPROVAL)
            # ================================================================
            print(f"\n{'='*80}")
            print("PHASE 4: MAPPING CONFIRMATION")
            print(f"{'='*80}")

            self.approved_mappings = self.ui.auto_approve_high_confidence(
                validated_proposals, threshold=auto_approve_threshold
            )

            if not self.approved_mappings:
                raise ValueError("No mappings approved. Lower threshold or check data quality.")

            # ================================================================
            # PHASE 5: PREPARE FOR BIAS ANALYSIS
            # ================================================================
            print(f"\n{'='*80}")
            print("PHASE 5: PREPARATION FOR BIAS ANALYSIS")
            print(f"{'='*80}")

            # Build feature map (exclude outcome and excluded columns)
            feature_map = {}
            for column, mapping in self.approved_mappings.items():
                feature = mapping.get("feature")
                if feature and feature not in ["__outcome__", "__exclude__", None]:
                    feature_map[feature] = column

            self.engine._feature_map = feature_map
            self.engine._target_column = final_target

            print(f"   Target column: '{final_target}'")
            print(f"   Features for analysis:")
            for feature, column in feature_map.items():
                print(f"     • {feature:25} ← {column}")

            # ================================================================
            # PHASE 6: BIAS DETECTION & STATISTICAL DIAGNOSIS
            # ================================================================
            print(f"\n{'='*80}")
            print("PHASE 6: BIAS DETECTION & STATISTICAL DIAGNOSIS")
            print(f"{'='*80}")

            diagnostic_results = self._run_statistical_diagnosis(feature_map, final_target)

            print(f"\n   Initial Bias Score: {diagnostic_results['initial_bias_score']:.4f}")
            print(f"   Significant Biases: {diagnostic_results['significant_bias_count']}")

            if diagnostic_results.get("feature_tests"):
                print(f"\n   Statistical Tests:")
                for feature, test in diagnostic_results["feature_tests"].items():
                    sig = "SIGNIFICANT" if test.get("significant_bias") else "OK"
                    p_val = test.get("p_value", 1.0)
                    print(f"     • {feature:25} p={p_val:.6f}  {sig}")

            # ================================================================
            # PHASE 7: BIAS MITIGATION
            # ================================================================
            if diagnostic_results["requires_mitigation"]:
                print(f"\n{'='*80}")
                print("PHASE 7: BIAS MITIGATION")
                print(f"{'='*80}")

                print(f"   Applying multi-objective SMOTE optimization...")

                self.corrected_df = self.engine.transform_industry(
                    self.original_df, diagnostic_results
                )

                # Calculate final bias score
                final_bias = self.engine.score(self.corrected_df, final_target)
                diagnostic_results["final_bias_score"] = final_bias

                initial_bias = diagnostic_results["initial_bias_score"]
                improvement = ((initial_bias - final_bias) / initial_bias * 100) if initial_bias > 0 else 0

                print(f"\n   Bias Score: {initial_bias:.4f} → {final_bias:.4f}")
                print(f"   Improvement: {improvement:+.1f}%")

                # Validate results
                validation = self.engine.validate_industry_readiness(
                    self.original_df, self.corrected_df, diagnostic_results
                )

                print(f"\n   Data Integrity:")
                print(f"     • Records: {validation['data_integrity']['records_before']:,} → {validation['data_integrity']['records_after']:,}")
                print(f"     • Retention: {validation['data_integrity']['retention_rate']:.1f}%")

                if validation.get("fairness_improvement"):
                    print(f"\n   Fairness Improvements:")
                    for feature, imp in validation["fairness_improvement"].items():
                        icon = "✅" if imp > 0 else "⚠️"
                        print(f"     {icon} {feature:25} {imp:+.1f}%")

            else:
                print(f"\n{'='*80}")
                print("PHASE 7: NO MITIGATION NEEDED")
                print(f"{'='*80}")
                print("   No significant biases detected. Dataset is fair.")

                self.corrected_df = self.original_df.copy()
                diagnostic_results["final_bias_score"] = diagnostic_results["initial_bias_score"]
                validation = {
                    "fairness_improvement": {},
                    "data_integrity": {
                        "records_before": len(self.original_df),
                        "records_after": len(self.original_df),
                        "retention_rate": 100.0
                    }
                }

            # ================================================================
            # PHASE 8: RESULTS COMPILATION
            # ================================================================
            print(f"\n{'='*80}")
            print("PHASE 8: COMPILING RESULTS")
            print(f"{'='*80}")

            self.results = {
                "original_df": self.original_df,
                "corrected_df": self.corrected_df,
                "mappings": self.approved_mappings,
                "diagnostics": diagnostic_results,
                "validation": validation,
                "target_column": final_target,
                "feature_map": feature_map,
                "metadata": {
                    "domain": self.domain,
                    "jurisdiction": self.jurisdiction,
                    "timestamp": datetime.now().isoformat(),
                    "auto_approve_threshold": auto_approve_threshold
                }
            }

            print(f"   Results compiled successfully")

            # ================================================================
            # PHASE 9: SAVE RESULTS
            # ================================================================
            self._save_results()

            # ================================================================
            # PHASE 10: GENERATE VISUALIZATIONS & REPORTS
            # ================================================================
            print(f"\n{'='*80}")
            print("PHASE 10: GENERATING VISUALIZATIONS & REPORTS")
            print(f"{'='*80}")

            # Generate visualizations
            try:
                print("\n   Generating visualizations...")

                # Disparity comparison
                self.viz.plot_disparity_comparison(
                    self.original_df, self.corrected_df,
                    feature_map, final_target,
                    save_path="biasclean_results/disparity_comparison.png"
                )

                # Fairness improvements
                if validation.get("fairness_improvement"):
                    self.viz.plot_feature_improvements(
                        validation,
                        save_path="biasclean_results/fairness_improvements.png"
                    )

                # Data integrity
                self.viz.plot_data_integrity(
                    validation,
                    save_path="biasclean_results/data_integrity.png"
                )

                print(f"   Visualizations saved to biasclean_results/")

            except Exception as e:
                print(f"   Visualization generation failed: {str(e)}")

            # Generate reports
            try:
                print("\n   Generating reports...")

                # Console report
                self.reporter.print_console_report(self.results)

                # HTML report
                self.reporter.generate_html_report(
                    self.results,
                    output_file="biasclean_results/biasclean_report.html"
                )

            except Exception as e:
                print(f"   Report generation failed: {str(e)}")

            # ================================================================
            # FINAL SUCCESS MESSAGE
            # ================================================================
            print(f"\n{'='*80}")
            print("PIPELINE COMPLETED SUCCESSFULLY!")
            print(f"{'='*80}")
            print(f"   Results saved to: biasclean_results/")
            print(f"   Visualizations: disparity_comparison.png, fairness_improvements.png")
            print(f"   Reports: biasclean_report.html, pipeline_summary.json")
            print(f"   Corrected dataset: corrected_dataset.csv")
            print(f"{'='*80}\n")

            return self.results

        except Exception as e:
            print(f"\n{'='*80}")
            print(f"PIPELINE FAILED")
            print(f"{'='*80}")
            print(f"Error: {str(e)}")
            print(f"{'='*80}\n")
            raise

    def _run_statistical_diagnosis(self, feature_map: Dict, target_column: str) -> Dict:
        """Run comprehensive statistical tests for bias detection"""
        diagnostic_results = {
            "feature_tests": {},
            "significant_bias_count": 0,
            "requires_mitigation": False,
            "target_column_used": target_column
        }

        for feature, column in feature_map.items():
            try:
                # Create contingency table
                contingency = pd.crosstab(
                    self.original_df[column],
                    self.original_df[target_column]
                )

                # Choose appropriate test
                if contingency.shape == (2, 2):
                    _, p_value = fisher_exact(contingency)
                    test_used = "fisher-exact"
                else:
                    chi2, p_value, _, _ = chi2_contingency(contingency)
                    test_used = "chi-square"

                significant = p_value < 0.05

                diagnostic_results["feature_tests"][feature] = {
                    "p_value": p_value,
                    "significant_bias": significant,
                    "test_used": test_used,
                    "column_name": column
                }

                if significant:
                    diagnostic_results["significant_bias_count"] += 1

            except Exception as e:
                diagnostic_results["feature_tests"][feature] = {
                    "error": str(e),
                    "significant_bias": False,
                    "test_used": "failed"
                }

        # Calculate overall bias score
        diagnostic_results["initial_bias_score"] = self.engine.score(
            self.original_df, target_column
        )

        # Determine if mitigation is needed
        diagnostic_results["requires_mitigation"] = (
            diagnostic_results["significant_bias_count"] > 0 and
            diagnostic_results["initial_bias_score"] > 0.05
        )

        return diagnostic_results

    def _save_results(self):
        """Save all pipeline results to disk"""
        os.makedirs("biasclean_results", exist_ok=True)

        # Save corrected dataset
        if self.corrected_df is not None:
            self.corrected_df.to_csv("biasclean_results/corrected_dataset.csv", index=False)

        # Save feature mappings
        with open("biasclean_results/feature_mappings.json", "w") as f:
            json.dump(self.approved_mappings, f, indent=2, default=str)

        # Save pipeline summary
        summary = {
            "timestamp": datetime.now().isoformat(),
            "domain": self.domain,
            "jurisdiction": self.jurisdiction,
            "dataset_info": {
                "original_records": len(self.original_df),
                "corrected_records": len(self.corrected_df) if self.corrected_df is not None else len(self.original_df),
                "columns": len(self.original_df.columns),
                "target_column": self.results.get("target_column")
            },
            "analysis_results": {
                "features_analyzed": list(self.results.get("feature_map", {}).keys()),
                "significant_biases": self.results.get("diagnostics", {}).get("significant_bias_count", 0),
                "initial_bias_score": self.results.get("diagnostics", {}).get("initial_bias_score", 0),
                "final_bias_score": self.results.get("diagnostics", {}).get("final_bias_score", 0)
            },
            "fairness_improvements": self.results.get("validation", {}).get("fairness_improvement", {}),
            "data_integrity": self.results.get("validation", {}).get("data_integrity", {})
        }

        with open("biasclean_results/pipeline_summary.json", "w") as f:
            json.dump(summary, f, indent=2, default=str)

        print(f"   Results saved to biasclean_results/")

# ============================================================================
# 8. SAMPLE DATASET GENERATOR (For Testing)
# ============================================================================
def create_sample_justice_dataset(n_samples: int = 2000, seed: int = 42) -> pd.DataFrame:
    """
    Create a realistic justice dataset with inherent bias for testing

    Args:
        n_samples: Number of records to generate
        seed: Random seed for reproducibility

    Returns:
        DataFrame with biased justice data
    """
    np.random.seed(seed)

    print(f"\n{'='*80}")
    print(f"GENERATING SAMPLE JUSTICE DATASET")
    print(f"{'='*80}")
    print(f"   Records: {n_samples:,}")
    print(f"   Seed: {seed}")

    # Generate base demographics
    data = {
        "person_id": range(n_samples),
        "defendant_name": [f"Defendant_{i}" for i in range(n_samples)],
        "race": np.random.choice(
            ["White", "Black", "Hispanic", "Asian", "Other"],
            n_samples,
            p=[0.50, 0.30, 0.15, 0.04, 0.01]
        ),
        "age": np.random.randint(18, 75, n_samples),
        "sex": np.random.choice(["Male", "Female"], n_samples, p=[0.70, 0.30]),
        "priors_count": np.random.randint(0, 25, n_samples),
        "c_charge_desc": np.random.choice(
            ["Aggravated Assault", "Burglary", "Drug Possession", "Theft", "Battery", "Fraud"],
            n_samples
        ),
        "screening_date": pd.date_range("2020-01-01", periods=n_samples, freq="H")
    }

    df = pd.DataFrame(data)

    # Create biased recidivism outcome
    # Base rate: 35%
    base_rate = 0.35

    # Race-based bias (this is what BiasClean will detect and fix)
    race_biases = {
        "White": base_rate * 0.75,     # 26% recidivism (25% lower)
        "Black": base_rate * 1.50,     # 53% recidivism (50% higher) - SIGNIFICANT BIAS
        "Hispanic": base_rate * 1.15,  # 40% recidivism (15% higher)
        "Asian": base_rate * 0.70,     # 25% recidivism (30% lower)
        "Other": base_rate * 1.00      # 35% recidivism (baseline)
    }

    # Age-based bias
    def age_factor(age):
        if age < 25:
            return 1.30  # Young: 30% higher risk
        elif age < 40:
            return 1.00  # Middle: baseline
        elif age < 60:
            return 0.85  # Mature: 15% lower risk
        else:
            return 0.70  # Senior: 30% lower risk

    # Priors-based factor
    def priors_factor(priors):
        if priors == 0:
            return 0.60
        elif priors < 3:
            return 0.90
        elif priors < 6:
            return 1.10
        else:
            return 1.40

    # Generate recidivism with compound biases
    recidivism = []
    for idx, row in df.iterrows():
        # Compound probability
        prob = race_biases[row["race"]]
        prob *= age_factor(row["age"])
        prob *= priors_factor(row["priors_count"])

        # Add noise
        prob *= np.random.uniform(0.90, 1.10)

        # Clip to valid probability range
        prob = np.clip(prob, 0.05, 0.95)

        # Generate binary outcome
        recidivism.append(np.random.binomial(1, prob))

    df["two_year_recid"] = recidivism

    # Print bias statistics
    print(f"\n   Recidivism Rates by Race (Showing Bias):")
    recid_by_race = df.groupby("race")["two_year_recid"].mean().sort_values(ascending=False)
    for race, rate in recid_by_race.items():
        deviation = ((rate - base_rate) / base_rate) * 100
        print(f"     • {race:15} {rate:.1%}  (deviation: {deviation:+.0f}%)")

    print(f"\n   Overall Recidivism Rate: {df['two_year_recid'].mean():.1%}")
    print(f"   Max Disparity: {recid_by_race.max() - recid_by_race.min():.1%}")
    print(f"{'='*80}\n")

    return df

# ============================================================================
# 9. INTERACTIVE COLAB INTERFACE
# ============================================================================

def run_interactive_pipeline():
    """
    Interactive interface for Colab users
    Handles file upload and pipeline execution
    """
    from google.colab import files
    import io

    print(f"\n{'='*80}")
    print("UNIVERSAL BIASCLEAN - INTERACTIVE MODE")
    print(f"{'='*80}\n")

    # Option 1: Upload CSV or Option 2: Use sample data
    print("Choose data source:")
    print("  1. Upload your own CSV file")
    print("  2. Generate sample justice dataset")
    print()

    choice = input("Enter choice (1 or 2): ").strip()

    df = None
    file_name = None

    if choice == "1":
        print("\nPlease upload your CSV file...")
        uploaded = files.upload()

        if not uploaded:
            print("No file uploaded. Exiting.")
            return None

        file_name = list(uploaded.keys())[0]
        df = pd.read_csv(io.BytesIO(uploaded[file_name]))

        print(f"\nLoaded: {file_name}")
        print(f"   Records: {len(df):,}")
        print(f"   Columns: {len(df.columns)}")

        # Show column preview
        print(f"\n   Columns: {', '.join(df.columns[:10])}")
        if len(df.columns) > 10:
            print(f"   ... and {len(df.columns) - 10} more")

    elif choice == "2":
        n_samples = input("\nEnter number of samples (default: 2000): ").strip()
        n_samples = int(n_samples) if n_samples else 2000

        df = create_sample_justice_dataset(n_samples=n_samples)
        file_name = "sample_justice_data.csv"

    else:
        print("Invalid choice. Exiting.")
        return None

    # Optional: Specify target column
    print(f"\n{'='*80}")
    print("Target Column Selection")
    print(f"{'='*80}")
    print("Available columns:")
    for idx, col in enumerate(df.columns, 1):
        unique_count = df[col].nunique()
        dtype = df[col].dtype
        print(f"  {idx:2}. {col:30} (unique: {unique_count:5}, dtype: {dtype})")

    print()
    target_input = input("Enter target column name (or press Enter for auto-detection): ").strip()
    target_column = target_input if target_input else None

    # Optional: Auto-approval threshold
    print()
    threshold_input = input("Enter auto-approval threshold (0.0-1.0, default: 0.80): ").strip()

    try:
        threshold = float(threshold_input) if threshold_input else 0.80
        threshold = max(0.0, min(1.0, threshold))  # Clip to valid range
    except ValueError:
        print("Invalid threshold. Using default 0.80")
        threshold = 0.80

    # Domain selection
    print("\nSelect domain:")
    print("  1. Justice (default)")
    print("  2. Healthcare")
    domain_choice = input("Enter choice (1 or 2, default: 1): ").strip()

    if domain_choice == "2":
        domain = "health"
        jurisdiction = "NHS_England"
    else:
        domain = "justice"
        jurisdiction = "US"

    # Initialize and run pipeline
    print(f"\n{'='*80}")
    print(f"INITIALIZING {domain.upper()} BIASCLEAN PIPELINE")
    print(f"{'='*80}\n")

    pipeline = UniversalBiasClean(domain=domain, jurisdiction=jurisdiction)

    # Run pipeline
    results = pipeline.process_dataset(
        df=df,
        target_column=target_column,
        auto_approve_threshold=threshold
    )

    # Display summary in Colab-friendly format
    print("\n" + "="*80)
    print("QUICK RESULTS SUMMARY")
    print("="*80)

    diagnostics = results.get("diagnostics", {})
    validation = results.get("validation", {})

    print(f"\nPipeline completed successfully!")
    print(f"\nKey Metrics:")
    print(f"   • Initial Bias Score: {diagnostics.get('initial_bias_score', 0):.4f}")
    print(f"   • Final Bias Score: {diagnostics.get('final_bias_score', 0):.4f}")

    initial = diagnostics.get('initial_bias_score', 0)
    final = diagnostics.get('final_bias_score', 0)
    improvement = ((initial - final) / initial * 100) if initial > 0 else 0
    print(f"   • Overall Improvement: {improvement:+.1f}%")

    print(f"\nAll results saved to: biasclean_results/")
    print(f"   • Corrected dataset: corrected_dataset.csv")
    print(f"   • HTML report: biasclean_report.html")
    print(f"   • Visualizations: *.png files")

    return results

# ============================================================================
# 10. MAIN EXECUTION BLOCK
# ============================================================================

if __name__ == "__main__":
    print("""
    UNIVERSAL BIASCLEAN v2.0 - COLAB EDITION

    Hierarchical Taxonomy + Constraint Validation Framework
    """)

    print("\nAVAILABLE FUNCTIONS:")
    print("="*80)
    print("\n1. run_interactive_pipeline()")
    print("   → Interactive mode with file upload and guided configuration")
    print("\n2. create_sample_justice_dataset(n_samples=2000)")
    print("   → Generate sample dataset for testing")
    print("\n3. UniversalBiasClean(domain='justice', jurisdiction='US')")
    print("   → Direct pipeline initialization for programmatic use")
    print("\n" + "="*80)

    print("\nQUICK START:")
    print("-"*80)
    print("\n# Option A: Interactive Mode (Recommended for first-time users)")
    print("results = run_interactive_pipeline()")

    print("\n# Option B: Programmatic Mode (For advanced users)")
    print("df = create_sample_justice_dataset(n_samples=2000)")
    print("pipeline = UniversalBiasClean(domain='justice', jurisdiction='california')")
    print("results = pipeline.process_dataset(df=df, auto_approve_threshold=0.85)")

    print("\n# Option C: Use Your Own CSV")
    print("pipeline = UniversalBiasClean(domain='justice')")
    print("results = pipeline.process_dataset(file_path='your_data.csv')")

    print("\n" + "="*80)
    print("\nTO BEGIN: Run one of the commands above in a new cell\n")

# ============================================================================
# 11. UTILITY FUNCTIONS FOR COLAB
# ============================================================================

def quick_start_sample():
    """
    Quick start with sample data - one-line execution
    """
    print("QUICK START: Running BiasClean with sample data...\n")

    # Generate sample data
    df = create_sample_justice_dataset(n_samples=2000)

    # Run pipeline
    pipeline = UniversalBiasClean(domain="justice", jurisdiction="california")
    results = pipeline.process_dataset(
        df=df,
        auto_approve_threshold=0.80
    )

    return results

def quick_start_csv(file_path: str, target_column: str = None, domain: str = "justice"):
    """
    Quick start with your CSV file

    Args:
        file_path: Path to your CSV file
        target_column: Optional target column name
        domain: Domain for analysis (justice or health)

    Returns:
        Pipeline results
    """
    print(f"QUICK START: Running {domain} BiasClean on {file_path}...\n")

    pipeline = UniversalBiasClean(domain=domain)
    results = pipeline.process_dataset(
        file_path=file_path,
        target_column=target_column,
        auto_approve_threshold=0.80
    )

    return results

def display_results_summary(results: Dict):
    """
    Display formatted results summary in Colab

    Args:
        results: Pipeline results dictionary
    """
    from IPython.display import display, HTML

    diagnostics = results.get("diagnostics", {})
    validation = results.get("validation", {})
    mappings = results.get("mappings", {})

    # Create HTML summary
    html = f"""
    <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                padding: 20px; border-radius: 10px; color: white; margin: 20px 0;'>
        <h2 style='margin: 0 0 15px 0;'>BiasClean Results Summary</h2>
        <div style='background: rgba(255,255,255,0.1); padding: 15px; border-radius: 5px;'>
            <h3 style='margin-top: 0;'>Bias Reduction</h3>
            <p style='font-size: 24px; margin: 10px 0;'>
                <strong>{diagnostics.get('initial_bias_score', 0):.4f}</strong> →
                <strong>{diagnostics.get('final_bias_score', 0):.4f}</strong>
            </p>
            <p style='font-size: 18px;'>
                Improvement: <strong>{((diagnostics.get('initial_bias_score', 0) - diagnostics.get('final_bias_score', 0)) / diagnostics.get('initial_bias_score', 1) * 100) if diagnostics.get('initial_bias_score', 0) > 0 else 0:+.1f}%</strong>
            </p>
        </div>
        <div style='background: rgba(255,255,255,0.1); padding: 15px; border-radius: 5px; margin-top: 10px;'>
            <h3 style='margin-top: 0;'>Data Integrity</h3>
            <p style='margin: 5px 0;'>
                Retention Rate: <strong>{validation.get('data_integrity', {}).get('retention_rate', 100):.1f}%</strong>
            </p>
            <p style='margin: 5px 0;'>
                Records: <strong>{validation.get('data_integrity', {}).get('records_before', 0):,}</strong> →
                <strong>{validation.get('data_integrity', {}).get('records_after', 0):,}</strong>
            </p>
        </div>
        <div style='background: rgba(255,255,255,0.1); padding: 15px; border-radius: 5px; margin-top: 10px;'>
            <h3 style='margin-top: 0;'>Features Analyzed</h3>
            <p style='margin: 5px 0;'>
                Mapped Features: <strong>{len(mappings)}</strong>
            </p>
            <p style='margin: 5px 0;'>
                Significant Biases: <strong>{diagnostics.get('significant_bias_count', 0)}</strong>
            </p>
        </div>
    </div>
    """

    display(HTML(html))

    # Display fairness improvements
    improvements = validation.get("fairness_improvement", {})
    if improvements:
        print("\nFAIRNESS IMPROVEMENTS BY FEATURE")
        print("="*80)

        for feature, imp_value in improvements.items():
            icon = "✅" if imp_value > 0 else "⚠️"
            bar_length = int(abs(imp_value) / 2)
            bar = "█" * min(bar_length, 50)

            print(f"{icon} {feature:25} {imp_value:+6.1f}% {bar}")

def download_results():
    """
    Download all results files from Colab to local machine
    """
    from google.colab import files
    import os
    import zipfile

    print("Preparing results for download...\n")

    # Create zip file
    zip_path = "biasclean_results.zip"

    with zipfile.ZipFile(zip_path, 'w') as zipf:
        if os.path.exists("biasclean_results"):
            for root, dirs, file_list in os.walk("biasclean_results"):
                for file in file_list:
                    file_path = os.path.join(root, file)
                    zipf.write(file_path, os.path.basename(file_path))

    print(f"Created: {zip_path}")
    print(f"   Contains: corrected_dataset.csv, HTML report, visualizations, and more")
    print("\nDownloading...")

    files.download(zip_path)

    print("Download complete!")

def compare_datasets(original_df: pd.DataFrame, corrected_df: pd.DataFrame,
                    target_column: str, feature_column: str):
    """
    Interactive comparison of original vs corrected datasets

    Args:
        original_df: Original dataframe
        corrected_df: Corrected dataframe
        target_column: Outcome variable
        feature_column: Protected attribute to compare
    """
    import matplotlib.pyplot as plt
    import seaborn as sns

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Original rates
    original_rates = original_df.groupby(feature_column)[target_column].mean()
    original_rates.plot(kind='bar', ax=axes[0], color='#e74c3c', alpha=0.7)
    axes[0].set_title('Before BiasClean', fontsize=14, fontweight='bold')
    axes[0].set_ylabel('Recidivism Rate', fontsize=12)
    axes[0].axhline(y=original_df[target_column].mean(), color='black',
                   linestyle='--', label='Overall Mean', alpha=0.5)
    axes[0].legend()
    axes[0].set_ylim(0, 1)

    # Corrected rates
    corrected_rates = corrected_df.groupby(feature_column)[target_column].mean()
    corrected_rates.plot(kind='bar', ax=axes[1], color='#27ae60', alpha=0.7)
    axes[1].set_title('After BiasClean', fontsize=14, fontweight='bold')
    axes[1].set_ylabel('Recidivism Rate', fontsize=12)
    axes[1].axhline(y=corrected_df[target_column].mean(), color='black',
                   linestyle='--', label='Overall Mean', alpha=0.5)
    axes[1].legend()
    axes[1].set_ylim(0, 1)

    plt.tight_layout()
    plt.show()

    # Print statistics
    print("\nSTATISTICAL COMPARISON")
    print("="*80)
    print(f"\nFeature: {feature_column}")
    print(f"Target: {target_column}\n")

    print(f"{'Group':<20} {'Before':<15} {'After':<15} {'Change':<15}")
    print("-"*65)

    for group in original_rates.index:
        before = original_rates.get(group, 0)
        after = corrected_rates.get(group, 0)
        change = after - before

        print(f"{str(group):<20} {before:<15.3f} {after:<15.3f} {change:+.3f}")

    # Calculate disparity reduction
    before_disparity = original_rates.max() - original_rates.min()
    after_disparity = corrected_rates.max() - corrected_rates.min()
    disparity_reduction = ((before_disparity - after_disparity) / before_disparity * 100)

    print("\n" + "="*80)
    print(f"Disparity (max-min): {before_disparity:.3f} → {after_disparity:.3f}")
    print(f"Disparity Reduction: {disparity_reduction:+.1f}%")
    print("="*80)

# ============================================================================
# 12. ADVANCED FEATURES
# ============================================================================

def batch_process_datasets(file_paths: List[str],
                          output_dir: str = "batch_results") -> Dict[str, Any]:
    """
    Process multiple datasets in batch mode

    Args:
        file_paths: List of CSV file paths
        output_dir: Directory for batch results

    Returns:
        Dictionary with results for each dataset
    """
    os.makedirs(output_dir, exist_ok=True)

    batch_results = {}

    print(f"\n{'='*80}")
    print(f"BATCH PROCESSING: {len(file_paths)} datasets")
    print(f"{'='*80}\n")

    for idx, file_path in enumerate(file_paths, 1):
        print(f"\n[{idx}/{len(file_paths)}] Processing: {file_path}")
        print("-"*80)

        try:
            # Create sub-directory for this dataset
            dataset_name = os.path.splitext(os.path.basename(file_path))[0]
            dataset_dir = os.path.join(output_dir, dataset_name)
            os.makedirs(dataset_dir, exist_ok=True)

            # Run pipeline
            pipeline = UniversalBiasClean(domain="justice")
            results = pipeline.process_dataset(
                file_path=file_path,
                auto_approve_threshold=0.80
            )

            # Save to dataset-specific directory
            results["corrected_df"].to_csv(
                os.path.join(dataset_dir, "corrected_dataset.csv"),
                index=False
            )

            # Store results
            batch_results[dataset_name] = {
                "success": True,
                "initial_bias": results["diagnostics"]["initial_bias_score"],
                "final_bias": results["diagnostics"]["final_bias_score"],
                "improvement": ((results["diagnostics"]["initial_bias_score"] -
                               results["diagnostics"]["final_bias_score"]) /
                               results["diagnostics"]["initial_bias_score"] * 100),
                "output_dir": dataset_dir
            }

            print(f"Success: {dataset_name}")

        except Exception as e:
            print(f"Failed: {file_path}")
            print(f"   Error: {str(e)}")

            batch_results[dataset_name] = {
                "success": False,
                "error": str(e)
            }

    # Generate batch summary
    print(f"\n{'='*80}")
    print("BATCH PROCESSING SUMMARY")
    print(f"{'='*80}\n")

    successful = sum(1 for r in batch_results.values() if r.get("success"))
    failed = len(batch_results) - successful

    print(f"Total Datasets: {len(batch_results)}")
    print(f"Successful:  {successful}")
    print(f"Failed:      {failed}")

    if successful > 0:
        avg_improvement = np.mean([
            r["improvement"] for r in batch_results.values()
            if r.get("success")
        ])
        print(f"\nAverage Improvement: {avg_improvement:+.1f}%")

    # Save batch summary
    with open(os.path.join(output_dir, "batch_summary.json"), "w") as f:
        json.dump(batch_results, f, indent=2, default=str)

    print(f"\nBatch summary saved to: {output_dir}/batch_summary.json")
    print(f"{'='*80}\n")

    return batch_results

def export_to_format(results: Dict, format: str = "all"):
    """
    Export results to various formats

    Args:
        results: Pipeline results
        format: 'csv', 'json', 'excel', or 'all'
    """
    corrected_df = results.get("corrected_df")

    if corrected_df is None:
        print("No corrected dataset found in results")
        return

    os.makedirs("biasclean_results/exports", exist_ok=True)

    formats_to_export = []
    if format == "all":
        formats_to_export = ["csv", "json", "excel"]
    else:
        formats_to_export = [format]

    print(f"\nEXPORTING RESULTS")
    print("="*80)

    for fmt in formats_to_export:
        try:
            if fmt == "csv":
                path = "biasclean_results/exports/corrected_dataset.csv"
                corrected_df.to_csv(path, index=False)
                print(f"CSV:   {path}")

            elif fmt == "json":
                path = "biasclean_results/exports/corrected_dataset.json"
                corrected_df.to_json(path, orient="records", indent=2)
                print(f"JSON:  {path}")

            elif fmt == "excel":
                path = "biasclean_results/exports/corrected_dataset.xlsx"
                corrected_df.to_excel(path, index=False, engine='openpyxl')
                print(f"Excel: {path}")

        except Exception as e:
            print(f"{fmt.upper()} export failed: {str(e)}")

    print("="*80)

# ============================================================================
# 13. HELP & DOCUMENTATION
# ============================================================================

def show_help():
    """Display comprehensive help documentation"""

    help_text = """
UNIVERSAL BIASCLEAN v2.0 - HELP

QUICK START FUNCTIONS:

1. run_interactive_pipeline()
   - Guided interface with file upload and configuration
   - Best for first-time users

2. quick_start_sample()
   - One-line execution with generated sample data
   - Perfect for testing and demonstrations

3. quick_start_csv('your_file.csv')
   - One-line execution with your own CSV
   - Fast processing for known datasets

─────────────────────────────────────────────────────────────────────────────

ADVANCED USAGE:

# Initialize pipeline with custom settings
pipeline = UniversalBiasClean(
    domain='justice',              # Domain: 'justice', 'health', 'finance'
    jurisdiction='california'      # Jurisdiction for compliance
)

# Process dataset with options
results = pipeline.process_dataset(
    file_path='data.csv',          # OR df=your_dataframe
    target_column='outcome',       # Optional: auto-detected if None
    auto_approve_threshold=0.85    # Confidence threshold (0.0-1.0)
)

─────────────────────────────────────────────────────────────────────────────

UTILITY FUNCTIONS:

- display_results_summary(results)    - Show formatted results in Colab
- download_results()                  - Download all files as ZIP
- compare_datasets(orig, corr, ...)   - Visual comparison
- export_to_format(results, 'csv')    - Export to CSV/JSON/Excel
- batch_process_datasets([files])     - Process multiple files

─────────────────────────────────────────────────────────────────────────────

OUTPUT FILES (saved to biasclean_results/):

- corrected_dataset.csv          - Bias-mitigated dataset
- biasclean_report.html         - Comprehensive HTML report
- feature_mappings.json         - Column mapping decisions
- pipeline_summary.json         - Execution summary
- disparity_comparison.png      - Before/after visualization
- fairness_improvements.png     - Feature improvements chart
- data_integrity.png            - Data retention metrics

─────────────────────────────────────────────────────────────────────────────

EXAMPLES:

# Example 1: Interactive mode
results = run_interactive_pipeline()

# Example 2: Quick test with sample data
results = quick_start_sample()
display_results_summary(results)

# Example 3: Custom configuration
pipeline = UniversalBiasClean(domain='justice', jurisdiction='florida')
df = pd.read_csv('compas_data.csv')
results = pipeline.process_dataset(
    df=df,
    target_column='two_year_recid',
    auto_approve_threshold=0.90
)

# Example 4: Compare results
compare_datasets(
    results['original_df'],
    results['corrected_df'],
    target_column='two_year_recid',
    feature_column='race'
)

─────────────────────────────────────────────────────────────────────────────

PIPELINE PHASES:

1. Dataset Loading          - Load CSV or DataFrame
2. Hierarchical Mapping     - 3-tier feature detection
3. Constraint Validation    - Statistical quality checks
4. Mapping Confirmation     - Auto-approval based on confidence
5. Preparation              - Build feature map for analysis
6. Bias Detection           - Statistical tests (Fisher's, Chi-square)
7. Bias Mitigation          - SMOTE-based rebalancing
8. Results Compilation      - Package all outputs
9. Save Results             - Write files to disk
10. Generate Reports        - HTML reports and visualizations

─────────────────────────────────────────────────────────────────────────────

KEY FEATURES:

✓ Hierarchical Taxonomy:     Universal → Domain → Jurisdiction
✓ Constraint Validation:     Statistical quality checks
✓ Auto-Approval:            Confidence-based mapping
✓ SMOTE Optimization:       Multi-objective bias mitigation
✓ Comprehensive Reports:    HTML, JSON, visualizations
✓ Legal Compliance:         GDPR, ECOA, Loomis standards
✓ Batch Processing:         Multiple datasets at once
✓ Colab Integration:        Upload, process, download

─────────────────────────────────────────────────────────────────────────────

MORE INFORMATION:

- GitHub: [Your repo URL]
- Documentation: [Your docs URL]
- Paper: "Universal BiasClean: Hierarchical Taxonomy + Constraint Validation"
"""

    print(help_text)

# ============================================================================
# 14. INITIALIZATION & USAGE INSTRUCTIONS
# ============================================================================

def show_welcome():
    """Display welcome message"""
    print("""
    ============================================================================
    UNIVERSAL BIASCLEAN v2.1 - WEIGHT-PRIORITIZED EDITION
    ============================================================================

    SUCCESSFULLY LOADED!

    Available Functions:
    1. run_interactive_pipeline()   - Interactive mode with upload
    2. quick_start_sample()         - Test with sample data
    3. quick_start_csv(file_path)   - Process your CSV file

    Quick Start (run in a new cell):
        results = quick_start_sample()          # Test with sample data
        results = run_interactive_pipeline()    # Interactive mode

    For help: show_help()
    ============================================================================
    """)

# Show welcome message
show_welcome()

# ============================================================================
# 15. EXECUTION GUARD (Only runs if file is executed directly)
# ============================================================================

if __name__ == "__main__":
    print("\nExecuting BiasClean Pipeline in interactive mode...\n")
    results = run_interactive_pipeline()